"""
VLMæœåŠ¡ - é‡æ„ç‰ˆæœ¬
æä¾›è§†è§‰æ¨¡å‹çš„å›¾åƒåˆ†æåŠŸèƒ½
ç»§æ‰¿OpenAICompatibleServiceä»¥å¤ç”¨é€šç”¨é€»è¾‘
"""

import json
import time
import asyncio
from typing import Optional, Dict, Any, List, Callable
import httpx
from .openai_base import OpenAICompatibleService, filter_thinking_content
from ..utils.common import (
    format_api_error, preprocess_image, check_multi_image_support, ProgressBar,
    log_complete, log_error,
    PREFIX, PROCESS_PREFIX, WARN_PREFIX, ERROR_PREFIX, format_elapsed_time,
    TASK_IMAGE_CAPTION, TASK_VIDEO_CAPTION
)
from .thinking_control import build_thinking_suppression


class VisionService(OpenAICompatibleService):
    """
    è§†è§‰æ¨¡å‹æœåŠ¡
    æ”¯æŒå•å›¾å’Œå¤šå›¾åˆ†æ
    """
    
    @staticmethod
    def _get_config() -> Dict[str, Any]:
        """è·å–è§†è§‰æ¨¡å‹é…ç½®"""
        from ..config_manager import config_manager
        config = config_manager.get_vision_config()
        current_provider = config.get('provider')

        if 'providers' in config and current_provider in config['providers']:
            provider_config = config['providers'][current_provider]
            return {
                'provider': current_provider,
                'model': provider_config.get('model', ''),
                'base_url': provider_config.get('base_url', ''),
                'api_key': provider_config.get('api_key', ''),
                'temperature': provider_config.get('temperature', 0.7),
                'top_p': provider_config.get('top_p', 0.9),
                'max_tokens': provider_config.get('max_tokens', 2000),
                'send_temperature': provider_config.get('send_temperature', True),
                'send_top_p': provider_config.get('send_top_p', True),
                'send_max_tokens': provider_config.get('send_max_tokens', True),
                'custom_params': provider_config.get('custom_params', ''),
                'auto_unload': provider_config.get('auto_unload', True)
            }
        else:
            return config
    
    @staticmethod
    async def _call_ollama_native_vision(
        model: str,
        system_prompt: str,
        images_b64: List[str],
        temperature: float,
        top_p: float,
        max_tokens: int,
        base_url: str,
        send_temperature: bool = True,
        send_top_p: bool = True,
        send_max_tokens: bool = True,
        stream_callback: Optional[Callable[[str], None]] = None,
        request_id: Optional[str] = None,
        is_multi: bool = False,
        auto_unload: bool = True,
        enable_advanced_params: bool = False,
        thinking_extra: Optional[Dict[str, Any]] = None,
        cancel_event: Optional[Any] = None,
        task_type: str = None,
        source: str = None
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨OllamaåŸç”Ÿè§†è§‰API (/api/chat)
        æ”¯æŒå•å›¾å’Œå¤šå›¾åˆ†æ
        
        å‚æ•°:
            enable_advanced_params: æ˜¯å¦å‘é€é«˜çº§å‚æ•°(temperature/top_p/num_predict)
            thinking_extra: æ€ç»´é“¾æ§åˆ¶å‚æ•°
        """
        from ..server import is_streaming_progress_enabled
        
        try:
            start_time = time.perf_counter()
            
            _thinking_extra = thinking_extra  # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
            _thinking_tag = "ğŸ’­" if _thinking_extra else ""
            
            # è®¡ç®—åŸºå‡† URL (ç¡®ä¿ç§»é™¤ /v1 å’Œæœ«å°¾æ–œæ )
            native_base = base_url.rstrip('/') if base_url else 'http://localhost:11434'
            if native_base.endswith('/v1'):
                native_base = native_base[:-3].rstrip('/')
            
            # åŠ¨æ€è®¡ç®—num_ctxï¼ˆæ ¹æ®å›¾åƒæ•°é‡ï¼‰
            # æ¯å¼ å›¾ç‰‡çº¦éœ€è¦1024-2048 tokens
            img_count = len(images_b64)
            
            # æ–‡æœ¬Tokenä¼°ç®— (0.6ç³»æ•°)
            prompt_ctx = int(len(system_prompt) * 0.6)
            
            # å›¾åƒTokenä¼°ç®— (æ¯å¼ 2048ä½œä¸ºåŸºå‡†)
            image_ctx = img_count * 2048
            
            # --- æ™ºèƒ½é¢„ç•™ç­–ç•¥ (é€‚é… Vision æ¨¡å‹) ---
            # å…³é”®ç‚¹ï¼šVisionæ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹åŒæ ·å ç”¨å¤§é‡ Output Token
            
            is_safe_standard_model = False
            if model:
                m = model.lower()
                if "instruct" in m or "chat" in m:
                    is_safe_standard_model = True

            if _thinking_extra or is_safe_standard_model:
                # å·²å…³é—­æ€ç»´é“¾ OR æ ‡å‡†æŒ‡ä»¤æ¨¡å‹ -> æè‡´èŠ‚çœæ¨¡å¼
                min_output = 512
                # å•å›¾å…è®¸è¿›ä¸€æ­¥ä¸‹æ¢è‡³ 2048ï¼Œå¤šå›¾ä¿æŒ 3072 èµ·æ­¥ä»¥ç¡®ä¿ç¨³å®š
                ctx_floor = 2048 if not is_multi else 3072
                sys_buffer = 384
            else:
                # æœªå…³é—­æ€ç»´é“¾ -> å®‰å…¨èƒ½å¤Ÿæ¨¡å¼
                min_output = 1024
                # å•å›¾ä¸‹é™ä» 4096 é™è‡³ 2048 (é€‚é… Ollama æ˜¾å­˜åˆ†é…ä¼˜åŒ–)
                ctx_floor = 2048 if not is_multi else 4096
                sys_buffer = 384 if not is_multi else 1024
            
            # è¾“å‡ºé¢„ç•™ (å¤šå›¾éœ€æ›´å¤š)
            # å¦‚æœæ˜¯å•å›¾æ¨¡å¼ï¼Œé¢„ç•™ 512 å·²è¶³å¤Ÿæè¿°ï¼›å¦‚æœæ˜¯å¤šå›¾ï¼Œä½¿ç”¨ min_output
            base_reserve = (img_count * 512) if is_multi else 512
            output_reserve = max(512 if not is_multi else min_output, base_reserve)
            
            required_ctx = prompt_ctx + image_ctx + output_reserve + sys_buffer
            
            # èŒƒå›´: [ctx_floor, 65536]
            num_ctx = max(ctx_floor, min(65536, required_ctx))
            num_ctx = ((num_ctx + 1023) // 1024) * 1024
            
            # [Debug] è¾“å‡ºå¤šå›¾è¯·æ±‚ä¿¡æ¯
            print(f"{PREFIX} ğŸ è§†è§‰è¯·æ±‚ | å›¾ç‰‡æ•°é‡:{len(images_b64)} | num_ctx:{num_ctx} | æ¨¡å‹:{model}")
            
            # æ„å»ºåŸºç¡€è¯·æ±‚ä½“
            payload = {
                "model": model,
                "messages": [{"role": "user", "content": system_prompt, "images": images_b64}],
                "stream": True
            }
            
            # ---æ„å»º options---
            # åŸºç¡€å‚æ•°ï¼šnum_ctxï¼ˆåŠ¨æ€ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼‰
            options = {
                "num_ctx": num_ctx
            }
            
            # é«˜çº§å‚æ•°ï¼šä»…åœ¨ç”¨æˆ·å¯ç”¨æ—¶å‘é€
            # å‚æ•°è¯´æ˜ï¼ˆåŸºäº Ollama å®˜æ–¹æ–‡æ¡£ï¼‰ï¼š
            # - temperature: æ§åˆ¶éšæœºæ€§ï¼Œé»˜è®¤0.8ï¼Œå€¼è¶Šä½è¾“å‡ºè¶Šç¨³å®š
            # - top_p: æ ¸é‡‡æ ·ï¼Œé»˜è®¤0.9ï¼Œé™åˆ¶å€™é€‰è¯æ¦‚ç‡èŒƒå›´
            # - num_predict: æœ€å¤§ç”ŸæˆTokenæ•°ï¼Œé»˜è®¤-1ï¼ˆæ— é™ï¼‰
            if enable_advanced_params:
                if send_temperature:
                    options["temperature"] = temperature
                if send_top_p:
                    options["top_p"] = top_p
                if send_max_tokens:
                    options["num_predict"] = max_tokens
            
            payload["options"] = options
            
            # æ·»åŠ æ€ç»´é“¾æ§åˆ¶å‚æ•°ï¼ˆå¦‚ think: true æˆ– think: falseï¼‰
            if _thinking_extra:
                payload.update(_thinking_extra)
            
            # è®¾ç½®è¶…æ—¶
            # åŸºç¡€è¯»å–è¶…æ—¶60ç§’ + æ¯å¼ å›¾ç‰‡å¢åŠ 30ç§’ + ä¸Šä¸‹æ–‡é•¿åº¦è‡ªé€‚åº”
            base_read_timeout = 60.0
            per_image_read_timeout = 30.0
            ctx_based_timeout = (num_ctx / 1000) * 2.0 # æ¯1000tokenså¢åŠ 2ç§’
            
            calculated_read_timeout = base_read_timeout + (img_count * per_image_read_timeout) + ctx_based_timeout
            
            # æœ€å¤§è¯»å–è¶…æ—¶é™åˆ¶ä¸º 10 åˆ†é’Ÿ (600s)
            final_read_timeout = min(600.0, max(60.0, calculated_read_timeout))
            
            # åˆ›å»ºç»Ÿä¸€è¿›åº¦æ¡ï¼ˆè‡ªåŠ¨å¤„ç†ç­‰å¾…â†’ç”Ÿæˆâ†’å®Œæˆçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼‰
            extra_info = f"Context:{num_ctx} | Timeout:{int(final_read_timeout)}s"
            pbar = ProgressBar(
                request_id=request_id,
                service_name="Ollama",
                extra_info=extra_info,
                streaming=is_streaming_progress_enabled(),
                task_type=task_type,
                source=source
            )
            
            start_time = time.perf_counter()
            
            # è·å–æŒä¹…åŒ–å®¢æˆ·ç«¯ä»¥æ”¯æŒè¿æ¥å¤ç”¨
            from .core import HTTPClientPool
            client = HTTPClientPool.get_client(
                provider="Ollama(Vision)",
                base_url=native_base,
                timeout=final_read_timeout
            )
            
            full_content = ""
            
            async def _request_core():
                nonlocal full_content
                async with client.stream('POST', f"{native_base}/api/chat", json=payload, follow_redirects=True) as resp:
                    if resp.status_code != 200:
                        error_text = await resp.aread()
                        pbar.error(f"Ollama API é”™è¯¯: {resp.status_code}")
                        try:
                            error_data = json.loads(error_text)
                            return {"success": False, "error": error_data.get('error', f'HTTP {resp.status_code}')}
                        except:
                            return {"success": False, "error": f'HTTP {resp.status_code}'}
                    
                    async for line in resp.aiter_lines():
                        if not line: continue
                        try:
                            chunk_data = json.loads(line)
                            message = chunk_data.get('message')
                            if message and isinstance(message, dict):
                                content = message.get('content', '') or ''
                                if not content.strip():
                                    thinking = message.get('thinking', '') or message.get('reasoning', '')
                                    if thinking and len(thinking.strip()) > 5:
                                        content = thinking
                                
                                if content and content.strip():
                                    full_content += content
                                    pbar.set_generating(len(full_content))
                                    pbar.update(len(full_content))
                                    if stream_callback: stream_callback(content)
                            
                            if chunk_data.get('done', False):
                                pbar.done(char_count=len(full_content), elapsed_ms=int((time.perf_counter() - start_time) * 1000))
                                break
                        except: continue
                return {"success": True, "content": full_content.strip()}

            # å®šä¹‰ç›‘è§†å™¨é€»è¾‘
            async def _monitor_interrupts(target_task):
                while not target_task.done():
                    is_interrupted = False
                    if cancel_event is not None and cancel_event.is_set():
                        is_interrupted = True
                    else:
                        try:
                            from server import PromptServer
                            if hasattr(PromptServer.instance, 'execution_interrupted') and PromptServer.instance.execution_interrupted:
                                is_interrupted = True
                        except: pass
                    
                    if is_interrupted:
                        target_task.cancel()
                        return True
                    await asyncio.sleep(0.1)
                return False

            # å¹¶å‘æ‰§è¡Œ
            req_task = asyncio.create_task(_request_core())
            monitor_task = asyncio.create_task(_monitor_interrupts(req_task))
            
            try:
                result = await req_task
                # å…œåº•å¤„ç†ï¼šç¡®ä¿å¤±è´¥ç»“æœæ—¶è¿›åº¦æ¡å·²åœæ­¢
                if not result.get("success") and not getattr(pbar, '_closed', False):
                    pbar.error(result.get("error", "æœªçŸ¥é”™è¯¯"))
                return result
            except Exception as req_err:
                if 'pbar' in locals() and pbar:
                    pbar.error(f"Ollama(Vision) è¯·æ±‚å¼‚å¸¸: {req_err}")
                return {"success": False, "error": f"Ollama(Vision) è¯·æ±‚å¼‚å¸¸: {req_err}"}
            except asyncio.CancelledError:

                # å…³é”®ä¿®å¤ï¼šç¡®ä¿è¿›åº¦æ¡åœ¨ç›‘è§†å™¨å–æ¶ˆæ—¶è¢«æ­£ç¡®æ¸…ç†
                pbar.cancel(f"{WARN_PREFIX} ä»»åŠ¡è¢«ä¸­æ–­ | æœåŠ¡:Ollama(Vision)")
                return {"success": False, "error": "ä»»åŠ¡è¢«ä¸­æ–­", "interrupted": True}
            finally:
                if not monitor_task.done(): monitor_task.cancel()
                # æ˜¾å­˜é‡Šæ”¾ä¿è¯ï¼šè§†è§‰èŠ‚ç‚¹å¯¹æ˜¾å­˜æ›´æ•æ„Ÿï¼Œå¿…é¡»ç¡®ä¿åœ¨æ‰€æœ‰é€€å‡ºè·¯å¾„æ‰§è¡Œ
                try:
                    from .llm import LLMService
                    await LLMService._unload_ollama_model(model, {"base_url": native_base, "auto_unload": auto_unload})
                except: pass
        
        # å…³é”®ä¿®å¤ï¼šå•ç‹¬æ•è·å¤–å±‚ CancelledErrorï¼Œç¡®ä¿ pbar è¢«æ­£ç¡®åœæ­¢
        except asyncio.CancelledError:
            if 'pbar' in locals() and pbar:
                pbar.cancel(f"{WARN_PREFIX} ä»»åŠ¡è¢«å¤–éƒ¨å–æ¶ˆ | æœåŠ¡:Ollama(Vision)")
            return {"success": False, "error": "ä»»åŠ¡è¢«å–æ¶ˆ", "interrupted": True}
        
        except Exception as e:
            # å…³é”®ä¿®å¤ï¼šç¡®ä¿ pbar åœ¨å¼‚å¸¸æ—¶ä¹Ÿè¢«åœæ­¢
            if 'pbar' in locals() and pbar:
                pbar.error(format_api_error(e, "Ollama"))
            return {"success": False, "error": format_api_error(e, "Ollama")}
    
    @staticmethod
    async def analyze_image(
        image_data: str,
        request_id: Optional[str] = None,
        stream_callback: Optional[Callable[[str], None]] = None,
        prompt_content: Optional[str] = None,
        custom_provider: Optional[str] = None,
        custom_provider_config: Optional[Dict[str, Any]] = None,
        cancel_event: Optional[Any] = None,
        task_type: str = None,
        source: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨è§†è§‰æ¨¡å‹åˆ†æå•å¼ å›¾åƒ
        
        å‚æ•°:
            image_data: å›¾åƒæ•°æ®ï¼ˆBase64ç¼–ç ï¼‰
            request_id: è¯·æ±‚ID
            stream_callback: æµå¼è¾“å‡ºå›è°ƒ
            prompt_content: è‡ªå®šä¹‰æç¤ºè¯
            custom_provider: è‡ªå®šä¹‰æœåŠ¡å•†
            custom_provider_config: è‡ªå®šä¹‰é…ç½®
        
        è¿”å›:
            Dict: {"success": bool, "data": {"description": str}, "error": str}
        """
        try:
            # è·å–é…ç½®
            if custom_provider and custom_provider_config:
                config = None
                provider = custom_provider
                api_key = custom_provider_config.get('api_key')
                model = custom_provider_config.get('model')
                temperature = custom_provider_config.get('temperature', 0.7)
                top_p = custom_provider_config.get('top_p', 0.9)
                max_tokens = custom_provider_config.get('max_tokens', 2000)
                send_temperature = custom_provider_config.get('send_temperature', True)
                send_top_p = custom_provider_config.get('send_top_p', True)
                send_max_tokens = custom_provider_config.get('send_max_tokens', True)
                base_url = custom_provider_config.get('base_url', '')
            else:
                config = VisionService._get_config()
                provider = config.get('provider', 'unknown')
                api_key = config.get('api_key')
                model = config.get('model')
                temperature = config.get('temperature', 0.7)
                top_p = config.get('top_p', 0.9)
                max_tokens = config.get('max_tokens', 2000)
                send_temperature = config.get('send_temperature', True)
                send_top_p = config.get('send_top_p', True)
                send_max_tokens = config.get('send_max_tokens', True)
                base_url = config.get('base_url', '')

            # æ³¨ï¼šå…è®¸ç©ºAPI Keyï¼Œæ”¯æŒæ— è®¤è¯æœåŠ¡å•†
            if not model:
                return {"success": False, "error": "æœªé…ç½®æ¨¡å‹åç§°"}

            provider_display_name = VisionService.get_provider_display_name(provider)

            from ..utils.common import REQUEST_PREFIX, PREFIX, format_model_with_thinking
            
            # æ£€æŸ¥æœåŠ¡é…ç½®ä»¥ç¡®å®šæ˜¯å¦æ˜¾ç¤ºæ€ç»´é“¾æ ‡è¯†
            from ..config_manager import config_manager
            service = config_manager.get_service(provider)
            disable_thinking_enabled = service.get('disable_thinking', True) if service else True
            # åªæœ‰å½“å¼€å…³å¼€å¯ä¸”æ¨¡å‹æ”¯æŒæ—¶æ‰æ˜¾ç¤ºæ ‡è¯†
            _thinking_check = build_thinking_suppression(provider, model) if disable_thinking_enabled else None
            thinking_disabled = _thinking_check is not None
            model_display = format_model_with_thinking(model, thinking_disabled)

            # é¢„å¤„ç†å›¾åƒ
            processed_image = preprocess_image(image_data, request_id=request_id)

            # è·å–ç³»ç»Ÿæç¤ºè¯
            system_prompt = prompt_content or "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€åœºæ™¯ã€é¢œè‰²ã€æ°›å›´ç­‰ã€‚"

            # Ollamaèµ°åŸç”ŸAPI (é€šè¿‡æœåŠ¡ç±»å‹åˆ¤æ–­)
            if service and service.get('type') == 'ollama':
                # è¯»å– Ollama æœåŠ¡çš„é…ç½®
                enable_advanced_params = service.get('enable_advanced_params', False)
                filter_thinking_output = service.get('filter_thinking_output', True)
                _ollama_thinking_extra = build_thinking_suppression(provider, model) if disable_thinking_enabled else None
                
                # æå–çº¯base64
                b64 = processed_image.split(',')[1] if ',' in processed_image else processed_image
                
                # æå‰è®¡ç®—auto_unloadé…ç½®
                native_base = base_url[:-3] if base_url.endswith('/v1') else (base_url or 'http://localhost:11434')
                native_base = native_base.rstrip('/')
                _cfg = {
                    'auto_unload': custom_provider_config.get('auto_unload', True) if custom_provider_config else config.get('auto_unload', True),
                    'base_url': native_base
                }
                auto_unload = _cfg['auto_unload']

                result = await VisionService._call_ollama_native_vision(
                    model=model,
                    system_prompt=system_prompt,
                    images_b64=[b64],
                    temperature=temperature,
                    top_p=top_p,
                    max_tokens=max_tokens,
                    base_url=base_url,
                    send_temperature=send_temperature,
                    send_top_p=send_top_p,
                    send_max_tokens=send_max_tokens,
                    stream_callback=stream_callback,
                    request_id=request_id,
                    is_multi=False,
                    auto_unload=auto_unload,
                    enable_advanced_params=enable_advanced_params,
                    thinking_extra=_ollama_thinking_extra,
                    cancel_event=cancel_event,
                    task_type=task_type or TASK_IMAGE_CAPTION,
                    source=source
                )
                
                if result["success"]:
                    # æ³¨ï¼šå¸è½½å·²åœ¨ _call_ollama_native_vision çš„ finally å—ä¸­å¤„ç†
                    
                    # åº”ç”¨æ€ç»´é“¾è¾“å‡ºè¿‡æ»¤
                    content = result["content"]
                    if filter_thinking_output:
                        content = filter_thinking_content(content)
                    
                    return {
                        "success": True,
                        "data": {"description": content}
                    }
                else:
                    return result

            # å…¶ä»–æœåŠ¡èµ°HTTPç›´è¿
            if not base_url:
                base_url = VisionService.get_provider_base_url(provider, custom_provider_config if custom_provider else None)
            
            # æ„å»ºæ¶ˆæ¯ï¼ˆå›¾åƒæ ¼å¼ï¼‰
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": system_prompt},
                        {"type": "image_url", "image_url": {"url": processed_image}}
                    ]
                }
            ]
            
            # æ£€æŸ¥disable_thinkingã€enable_advanced_paramså’Œfilter_thinking_outputé…ç½®
            from ..config_manager import config_manager
            service = config_manager.get_service(provider)
            disable_thinking_enabled = service.get('disable_thinking', True) if service else True
            enable_advanced_params = service.get('enable_advanced_params', False) if service else False
            filter_thinking_output = service.get('filter_thinking_output', True) if service else True
            
            debug_mode = None
            custom_params_text = None
            if custom_provider_config:
                debug_mode = custom_provider_config.get('debug_mode')
                custom_params_text = custom_provider_config.get('custom_params')
            if debug_mode is None:
                debug_mode = service.get('debug_mode', False) if service else False
            if custom_params_text is None:
                custom_params_text = config.get('custom_params', '') if config else ''
            
            custom_params = None
            if custom_params_text and str(custom_params_text).strip():
                try:
                    custom_params = json.loads(custom_params_text)
                    if not isinstance(custom_params, dict):
                        return {"success": False, "error": "è‡ªå®šä¹‰è¯·æ±‚å‚æ•°(JSON)å¿…é¡»æ˜¯å¯¹è±¡"}
                except Exception as e:
                    return {"success": False, "error": f"è‡ªå®šä¹‰è¯·æ±‚å‚æ•°(JSON)æ ¼å¼é”™è¯¯: {str(e)}"}
            thinking_extra = build_thinking_suppression(provider, model) if disable_thinking_enabled else None
            
            result = await VisionService._http_request_chat_completions(
                base_url=base_url,
                api_key=api_key,
                model=model,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                send_temperature=send_temperature,
                send_top_p=send_top_p,
                send_max_tokens=send_max_tokens,
                thinking_extra=thinking_extra,
                enable_advanced_params=enable_advanced_params,
                stream_callback=stream_callback,
                request_id=request_id,
                provider_display_name=provider_display_name,
                cancel_event=cancel_event,
                debug_mode=debug_mode,
                custom_request_params=custom_params,
                task_type=task_type or TASK_IMAGE_CAPTION,
                source=source
            )

            if result["success"]:
                # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åº”ç”¨æ€ç»´é“¾è¾“å‡ºè¿‡æ»¤
                content = result["content"]
                if filter_thinking_output:
                    content = filter_thinking_content(content)
                return {
                    "success": True,
                    "data": {"description": content}
                }
            else:
                return result

        except Exception as e:
            return {"success": False, "error": format_api_error(e, "VLMæœåŠ¡")}
    
    @staticmethod
    async def analyze_images(
        images_data: List[str],
        request_id: Optional[str] = None,
        stream_callback: Optional[Callable[[str], None]] = None,
        prompt_content: Optional[str] = None,
        custom_provider: Optional[str] = None,
        custom_provider_config: Optional[Dict[str, Any]] = None,
        cancel_event: Optional[Any] = None,
        task_type: str = None,
        source: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨è§†è§‰æ¨¡å‹åˆ†æå¤šå¼ å›¾åƒ
        
        å‚æ•°:
            images_data: å›¾åƒæ•°æ®åˆ—è¡¨ï¼ˆBase64ç¼–ç ï¼‰
            request_id: è¯·æ±‚ID
            stream_callback: æµå¼è¾“å‡ºå›è°ƒ
            prompt_content: è‡ªå®šä¹‰æç¤ºè¯
            custom_provider: è‡ªå®šä¹‰æœåŠ¡å•†
            custom_provider_config: è‡ªå®šä¹‰é…ç½®
        
        è¿”å›:
            Dict: {"success": bool, "data": {"description": str}, "error": str}
        """
        try:
            # è·å–é…ç½®
            if custom_provider and custom_provider_config:
                config = None
                provider = custom_provider
                api_key = custom_provider_config.get('api_key')
                model = custom_provider_config.get('model')
                temperature = custom_provider_config.get('temperature', 0.7)
                top_p = custom_provider_config.get('top_p', 0.9)
                max_tokens = custom_provider_config.get('max_tokens', 2000)
                send_temperature = custom_provider_config.get('send_temperature', True)
                send_top_p = custom_provider_config.get('send_top_p', True)
                send_max_tokens = custom_provider_config.get('send_max_tokens', True)
                base_url = custom_provider_config.get('base_url', '')
            else:
                config = VisionService._get_config()
                provider = config.get('provider', 'unknown')
                api_key = config.get('api_key')
                model = config.get('model')
                temperature = config.get('temperature', 0.7)
                top_p = config.get('top_p', 0.9)
                max_tokens = config.get('max_tokens', 2000)
                send_temperature = config.get('send_temperature', True)
                send_top_p = config.get('send_top_p', True)
                send_max_tokens = config.get('send_max_tokens', True)
                base_url = config.get('base_url', '')

            # æ³¨ï¼šå…è®¸ç©ºAPI Keyï¼Œæ”¯æŒæ— è®¤è¯æœåŠ¡å•†
            if not model:
                return {"success": False, "error": "æœªé…ç½®æ¨¡å‹åç§°"}

            provider_display_name = VisionService.get_provider_display_name(provider)

            from ..utils.common import REQUEST_PREFIX, PREFIX, format_model_with_thinking
            
            # æ£€æŸ¥æœåŠ¡é…ç½®ä»¥ç¡®å®šæ˜¯å¦æ˜¾ç¤ºæ€ç»´é“¾æ ‡è¯†
            from ..config_manager import config_manager
            service = config_manager.get_service(provider)
            disable_thinking_enabled = service.get('disable_thinking', True) if service else True
            # åªæœ‰å½“å¼€å…³å¼€å¯ä¸”æ¨¡å‹æ”¯æŒæ—¶æ‰æ˜¾ç¤ºæ ‡è¯†
            _thinking_check = build_thinking_suppression(provider, model) if disable_thinking_enabled else None
            thinking_disabled = _thinking_check is not None
            model_display = format_model_with_thinking(model, thinking_disabled)

            # æ£€æŸ¥æ˜¯å¦æ”¯æŒå¤šå›¾
            supports_multi, max_images = check_multi_image_support(provider, model)
            
            if not supports_multi:
                return {"success": False, "error": f"æ¨¡å‹ {model} ä¸æ”¯æŒå¤šå›¾åƒåˆ†æ"}
            
            if len(images_data) > max_images:
                return {"success": False, "error": f"å›¾åƒæ•°é‡ {len(images_data)} è¶…è¿‡æ¨¡å‹é™åˆ¶ {max_images}"}

            # é¢„å¤„ç†æ‰€æœ‰å›¾åƒï¼ˆæ™ºèƒ½å‹ç¼©ï¼šæ ¹æ®å›¾åƒæ•°é‡åŠ¨æ€è°ƒæ•´è´¨é‡ï¼‰
            img_count = len(images_data)
            from ..utils.common import get_optimal_image_params
            _, _, compression_level = get_optimal_image_params(img_count)
            
            # ä½¿ç”¨ ProgressBar ç®¡ç†é¢„å¤„ç†è¿›åº¦
            pbar = ProgressBar(request_id=request_id, service_name="å›¾åƒé¢„å¤„ç†", streaming=False)
            processed_images = []
            for idx, img in enumerate(images_data, 1):
                processed = preprocess_image(img, request_id=request_id, silent=True, image_count=img_count)
                processed_images.append(processed)
            
            pbar.done(f"{PREFIX} ğŸŸ¡ é¢„å¤„ç†å®Œæˆ: {img_count}/{img_count} | å‹ç¼©:{compression_level}")

            # è·å–ç³»ç»Ÿæç¤ºè¯
            system_prompt = prompt_content or "è¯·è¯¦ç»†æè¿°è¿™äº›å›¾ç‰‡ï¼Œåˆ†æå®ƒä»¬ä¹‹é—´çš„å…³ç³»å’Œå·®å¼‚ã€‚"

            # Ollamaèµ°åŸç”ŸAPI (é€šè¿‡æœåŠ¡ç±»å‹åˆ¤æ–­)
            if service and service.get('type') == 'ollama':
                # è¯»å– Ollama æœåŠ¡çš„é…ç½®
                from ..config_manager import config_manager
                # æ­¤å¤„ä¿æŒç±»å‹åˆ¤æ–­ï¼Œä¸å†ç¡¬ç¼–ç  ID 'ollama'
                disable_thinking_enabled = service.get('disable_thinking', True)
                enable_advanced_params = service.get('enable_advanced_params', False)
                filter_thinking_output = service.get('filter_thinking_output', True)
                _ollama_thinking_extra = build_thinking_suppression(provider, model) if disable_thinking_enabled else None
                
                # æå‰è®¡ç®—auto_unloadé…ç½®
                native_base = base_url[:-3] if base_url.endswith('/v1') else (base_url or 'http://localhost:11434')
                native_base = native_base.rstrip('/')
                _cfg = {
                    'auto_unload': custom_provider_config.get('auto_unload', True) if custom_provider_config else config.get('auto_unload', True),
                    'base_url': native_base
                }
                auto_unload = _cfg['auto_unload']

                # æå–çº¯base64
                b64_images = [img.split(',')[1] if ',' in img else img for img in processed_images]
                
                result = await VisionService._call_ollama_native_vision(
                    model=model,
                    system_prompt=system_prompt,
                    images_b64=b64_images,
                    temperature=temperature,
                    top_p=top_p,
                    max_tokens=max_tokens,
                    base_url=base_url,
                    send_temperature=send_temperature,
                    send_top_p=send_top_p,
                    send_max_tokens=send_max_tokens,
                    stream_callback=stream_callback,
                    request_id=request_id,
                    is_multi=True,
                    auto_unload=auto_unload,
                    enable_advanced_params=enable_advanced_params,
                    thinking_extra=_ollama_thinking_extra,
                    cancel_event=cancel_event,
                    task_type=task_type or TASK_VIDEO_CAPTION,
                    source=source
                )
                
                if result["success"]:
                    # æ³¨ï¼šå¸è½½å·²åœ¨ _call_ollama_native_vision çš„ finally å—ä¸­å¤„ç†
                    
                    # åº”ç”¨æ€ç»´é“¾è¾“å‡ºè¿‡æ»¤
                    content = result["content"]
                    if filter_thinking_output:
                        content = filter_thinking_content(content)
                    
                    return {
                        "success": True,
                        "data": {"description": content}
                    }
                else:
                    return result

            # å…¶ä»–æœåŠ¡èµ°HTTPç›´è¿
            if not base_url:
                base_url = VisionService.get_provider_base_url(provider, custom_provider_config if custom_provider else None)
            
            # æ„å»ºå¤šå›¾æ¶ˆæ¯
            content = [{"type": "text", "text": system_prompt}]
            for img in processed_images:
                content.append({"type": "image_url", "image_url": {"url": img}})
            
            messages = [{"role": "user", "content": content}]
            
            # æ£€æŸ¥disable_thinkingã€enable_advanced_paramså’Œfilter_thinking_outputé…ç½®
            from ..config_manager import config_manager
            service = config_manager.get_service(provider)
            disable_thinking_enabled = service.get('disable_thinking', True) if service else True
            enable_advanced_params = service.get('enable_advanced_params', False) if service else False
            filter_thinking_output = service.get('filter_thinking_output', True) if service else True
            
            debug_mode = None
            custom_params_text = None
            if custom_provider_config:
                debug_mode = custom_provider_config.get('debug_mode')
                custom_params_text = custom_provider_config.get('custom_params')
            if debug_mode is None:
                debug_mode = service.get('debug_mode', False) if service else False
            if custom_params_text is None:
                custom_params_text = config.get('custom_params', '') if config else ''
            
            custom_params = None
            if custom_params_text and str(custom_params_text).strip():
                try:
                    custom_params = json.loads(custom_params_text)
                    if not isinstance(custom_params, dict):
                        return {"success": False, "error": "è‡ªå®šä¹‰è¯·æ±‚å‚æ•°(JSON)å¿…é¡»æ˜¯å¯¹è±¡"}
                except Exception as e:
                    return {"success": False, "error": f"è‡ªå®šä¹‰è¯·æ±‚å‚æ•°(JSON)æ ¼å¼é”™è¯¯: {str(e)}"}
            thinking_extra = build_thinking_suppression(provider, model) if disable_thinking_enabled else None
            
            result = await VisionService._http_request_chat_completions(
                base_url=base_url,
                api_key=api_key,
                model=model,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                send_temperature=send_temperature,
                send_top_p=send_top_p,
                send_max_tokens=send_max_tokens,
                thinking_extra=thinking_extra,
                enable_advanced_params=enable_advanced_params,
                stream_callback=stream_callback,
                request_id=request_id,
                provider_display_name=provider_display_name,
                cancel_event=cancel_event,
                debug_mode=debug_mode,
                custom_request_params=custom_params,
                task_type=task_type or TASK_VIDEO_CAPTION,
                source=source
            )

            if result["success"]:
                # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åº”ç”¨æ€ç»´é“¾è¾“å‡ºè¿‡æ»¤
                content = result["content"]
                if filter_thinking_output:
                    content = filter_thinking_content(content)
                return {
                    "success": True,
                    "data": {"description": content}
                }
            else:
                return result

        except Exception as e:
            # ç¡®ä¿è¿›åº¦æ¡åœ¨å¼‚å¸¸æ—¶è¢«åœæ­¢
            if 'pbar' in locals() and pbar and not getattr(pbar, '_closed', False):
                pbar.error(format_api_error(e, "VLMæœåŠ¡"))
            return {"success": False, "error": format_api_error(e, "VLMæœåŠ¡")}
