import asyncio
import random
import time
import threading
import hashlib
import re

from comfy.model_management import InterruptProcessingException

from ..services.llm import LLMService
from ..utils.common import format_api_error, format_model_with_thinking, generate_request_id, log_prepare, log_error, TASK_EXPAND, SOURCE_NODE
from ..services.thinking_control import build_thinking_suppression
from .base import LLMNodeBase


class PromptExpand(LLMNodeBase):
    """
    æç¤ºè¯å¢å¼ºèŠ‚ç‚¹
    - è¾“å…¥"source_text"ï¼Œæ ¹æ®æ‰€é€‰è§„åˆ™æ¨¡æ¿æˆ–è‡ªå®šä¹‰è§„åˆ™è¿›è¡Œå¢å¼º/æ‰©å†™
    - ä»…åŒ…å«ä¸€ä¸ªå­—ç¬¦ä¸²è¾“å…¥å’Œä¸€ä¸ªå­—ç¬¦ä¸²è¾“å‡º
    """

    @classmethod
    def INPUT_TYPES(cls):
        # ä»config_managerè·å–ç³»ç»Ÿæç¤ºè¯é…ç½®
        from ..config_manager import config_manager
        system_prompts = config_manager.get_system_prompts()

        # è·å–æ‰€æœ‰expand_promptsä½œä¸ºä¸‹æ‹‰é€‰é¡¹
        expand_prompts = {}
        active_expand_id = None
        if system_prompts:
            expand_prompts = system_prompts.get('expand_prompts', {}) or {}
            active_expand_id = system_prompts.get('active_prompts', {}).get('expand')

        # æ„å»ºæç¤ºè¯æ¨¡æ¿é€‰é¡¹ï¼ˆæ”¯æŒåˆ†ç±»æ ¼å¼ï¼šç±»åˆ«/è§„åˆ™åç§°ï¼‰
        prompt_template_options = []
        id_to_display_name = {}
        for key, value in expand_prompts.items():
            # è¿‡æ»¤æ‰ä¸åœ¨åç«¯æ˜¾ç¤ºçš„è§„åˆ™
            show_in = value.get('showIn', ["frontend", "node"])
            if 'node' not in show_in:
                continue

            name = value.get('name', key)
            category = value.get('category', '')
            # å¦‚æœæœ‰åˆ†ç±»ï¼Œæ˜¾ç¤ºä¸º "ç±»åˆ«/è§„åˆ™åç§°"ï¼Œå¦åˆ™ç›´æ¥æ˜¾ç¤ºè§„åˆ™åç§°
            display_name = f"{category}/{name}" if category else name
            id_to_display_name[key] = display_name
            prompt_template_options.append(display_name)

        # é»˜è®¤é€‰é¡¹å›é€€
        default_template_name = prompt_template_options[0] if prompt_template_options else "æ‰©å†™-è‡ªç„¶è¯­è¨€"
        if active_expand_id and active_expand_id in id_to_display_name:
            default_template_name = id_to_display_name[active_expand_id]
        
        # ---åŠ¨æ€è·å–LLMæœåŠ¡/æ¨¡å‹åˆ—è¡¨---
        service_options = cls.get_llm_service_options()
        default_service = service_options[0] if service_options else "æ™ºè°±"

        return {
            "required": {
                # è§„åˆ™æ¨¡æ¿ï¼šæ¥è‡ªç³»ç»Ÿé…ç½®çš„æ‰€æœ‰æ‰©å†™è§„åˆ™
                "rule": (prompt_template_options or ["æ‰©å†™-è‡ªç„¶è¯­è¨€"], {"default": default_template_name, "tooltip": "Choose a preset rule for prompt enhancement"}),
                # ä¸´æ—¶è§„åˆ™å¼€å…³
                "custom_rule": ("BOOLEAN", {"default": False, "label_on": "Enable", "label_off": "Disable", "tooltip": "Enable to use custom rule content below instead of preset"}),
                # ä¸´æ—¶è§„åˆ™å†…å®¹è¾“å…¥æ¡†
                "custom_rule_content": ("STRING", {"multiline": True, "default": "", "placeholder": "åœ¨æ­¤è¾“å…¥ä¸´æ—¶è§„åˆ™ï¼Œä»…åœ¨å¯ç”¨'ä¸´æ—¶è§„åˆ™'æ—¶ç”Ÿæ•ˆ", "tooltip": "åœ¨æ­¤è¾“å…¥æ‚¨çš„è‡ªå®šä¹‰è§„åˆ™å†…å®¹; ğŸ’¡è¾“å…¥è§¦å‘è¯[R],å¯ä»¥è®©èŠ‚ç‚¹æ¯æ¬¡éƒ½è¢«æ‰§è¡Œ"}),
                # ç”¨æˆ·æç¤ºè¯
                "user_prompt": ("STRING", {"multiline": True, "default": "", "placeholder": "å¡«å†™çš„è¦ä¼˜åŒ–çš„æç¤ºè¯åŸæ–‡ï¼Œè‹¥å­˜åœ¨åŸæ–‡ç«¯å£è¾“å…¥å’Œå†…å®¹è¾“å…¥ï¼Œå°†åˆå¹¶æäº¤", "tooltip": "æƒ³è¦å¢å¼ºçš„åŸå§‹æç¤ºè¯; ğŸ’¡è¾“å…¥è§¦å‘è¯[R],å¯ä»¥è®©èŠ‚ç‚¹æ¯æ¬¡éƒ½è¢«æ‰§è¡Œ"}),
                # æ‰©å†™æœåŠ¡
                "llm_service": (service_options, {"default": default_service, "tooltip": "Select LLM service and model"}),
                # Ollamaè‡ªåŠ¨é‡Šæ”¾æ˜¾å­˜
                "ollama_auto_unload": ("BOOLEAN", {"default": True, "label_on": "Enable", "label_off": "Disable", "tooltip": "Auto unload Ollama model after generation"}),
            },
            "optional": {
                # åŸæ–‡è¾“å…¥ç«¯å£
                "source_text": ("STRING", {"default": "", "multiline": True, "defaultInput": True, "placeholder": "Input text to enhance...", "tooltip": "å¯é€‰çš„è¾“å…¥æ–‡æœ¬; ğŸ’¡è¾“å…¥è§¦å‘è¯[R],å¯ä»¥è®©èŠ‚ç‚¹æ¯æ¬¡éƒ½è¢«æ‰§è¡Œ"}),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("enhanced_text",)
    FUNCTION = "enhance"
    CATEGORY = "âœ¨Prompt Assistant"
    OUTPUT_NODE = False

    @classmethod
    def IS_CHANGED(cls, rule=None, custom_rule=None, custom_rule_content=None, user_prompt=None, llm_service=None, ollama_auto_unload=None, source_text=None, unique_id=None):
        """
        åªåœ¨è¾“å…¥å†…å®¹çœŸæ­£å˜åŒ–æ—¶æ‰è§¦å‘é‡æ–°æ‰§è¡Œ
        ä½¿ç”¨è¾“å…¥å‚æ•°çš„å“ˆå¸Œå€¼ä½œä¸ºåˆ¤æ–­ä¾æ®
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¼ºåˆ¶åˆ·æ–°ç¬¦å· [R]
        if cls._check_is_changed_bypass(rule, custom_rule_content, user_prompt, source_text):
            return float("nan")

        text_hash = hashlib.md5(((source_text or "")).encode('utf-8')).hexdigest()
        temp_rule_hash = hashlib.md5((custom_rule_content or "").encode('utf-8')).hexdigest()
        user_hint_hash = hashlib.md5((user_prompt or "").encode('utf-8')).hexdigest()

        input_hash = hash((
            rule,
            bool(custom_rule),
            temp_rule_hash,
            user_hint_hash,
            llm_service,
            bool(ollama_auto_unload),
            text_hash,
        ))
        return input_hash

    def enhance(self, rule, custom_rule, custom_rule_content, user_prompt, llm_service, ollama_auto_unload, source_text=None, unique_id=None):
        """
        å¢å¼º/æ‰©å†™æ–‡æœ¬å‡½æ•°
        """
        try:
            # å…è®¸åŸæ–‡ä¸ºç©ºï¼Œä½†åŸæ–‡ä¸ç”¨æˆ·æç¤ºè¯è‡³å°‘æœ‰ä¸€é¡¹éç©º
            source_text = (source_text or "").strip()
            user_prompt = (user_prompt or "").strip()
            if not source_text and not user_prompt:
                return ("",)

            # å‡†å¤‡ç³»ç»Ÿæç¤ºè¯ï¼ˆè§„åˆ™ï¼‰
            system_message = None
            rule_name = "Custom Rule" if (custom_rule and custom_rule_content) else rule

            if custom_rule and custom_rule_content:
                # ä½¿ç”¨ä¸´æ—¶è§„åˆ™
                system_message = {"role": "system", "content": custom_rule_content}
            else:
                # ä½¿ç”¨æ¨¡æ¿ï¼šä»config_managerè·å–ç³»ç»Ÿæç¤ºè¯é…ç½®
                from ..config_manager import config_manager
                system_prompts = config_manager.get_system_prompts()
                expand_prompts = system_prompts.get('expand_prompts', {}) if system_prompts else {}

                # æŸ¥æ‰¾é€‰å®šçš„æç¤ºè¯æ¨¡æ¿ï¼ˆæŒ‰æ˜¾ç¤ºåç§°åŒ¹é…ï¼‰
                # æ˜¾ç¤ºåç§°æ ¼å¼ï¼šæœ‰åˆ†ç±»æ—¶ä¸º "ç±»åˆ«/è§„åˆ™åç§°"ï¼Œæ— åˆ†ç±»æ—¶ä¸º "è§„åˆ™åç§°"
                template_found = False
                for key, value in expand_prompts.items():
                    name = value.get('name', key)
                    category = value.get('category', '')
                    # æ„å»ºä¸ä¸‹æ‹‰åˆ—è¡¨ä¸€è‡´çš„æ˜¾ç¤ºåç§°
                    display_name = f"{category}/{name}" if category else name
                    if display_name == rule:
                        system_message = {"role": value.get('role', 'system'), "content": value.get('content', '')}
                        template_found = True
                        break
                if not template_found:
                    # å…è®¸ç”¨è§„åˆ™åç§°æˆ–é”®åç›´æ¥åŒ¹é…ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
                    for key, value in expand_prompts.items():
                        if value.get('name') == rule or key == rule:
                            system_message = {"role": value.get('role', 'system'), "content": value.get('content', '')}
                            template_found = True
                            break
                if not template_found or not system_message or not system_message.get('content'):
                    # å›é€€åˆ°é»˜è®¤
                    system_message = {"role": "system", "content": "ä½ æ˜¯ä¸€åæç¤ºè¯æ‰©å†™ä¸“å®¶ï¼Œè¯·å°†ç”¨æˆ·ç»™å®šæ–‡æœ¬æ‰©å†™ä¸ºæ›´å®Œæ•´ã€æ›´å…·å¯è¯»æ€§å’Œå¯æ‰§è¡Œæ€§çš„æç¤ºè¯ã€‚"}
                    rule_name = "Default Rule"

            # ---è§£ææœåŠ¡/æ¨¡å‹å­—ç¬¦ä¸²---
            service_id, model_name = self.parse_service_model(llm_service)
            if not service_id:
                raise ValueError(f"Invalid service selection: {llm_service}")
            
            # ---è·å–æœåŠ¡é…ç½®---
            from ..config_manager import config_manager
            service = config_manager.get_service(service_id)
            if not service:
                raise ValueError(f"Service config not found: {llm_service}")
            
            # ---æ„å»ºprovider_config---
            # æŸ¥æ‰¾æŒ‡å®šçš„æ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
            llm_models = service.get('llm_models', [])
            target_model = None
            
            if model_name:
                # æŸ¥æ‰¾æŒ‡å®šçš„æ¨¡å‹
                target_model = next((m for m in llm_models if m.get('name') == model_name), None)
            
            if not target_model:
                # ä½¿ç”¨é»˜è®¤æ¨¡å‹æˆ–ç¬¬ä¸€ä¸ªæ¨¡å‹
                target_model = next((m for m in llm_models if m.get('is_default')), 
                                    llm_models[0] if llm_models else None)
            
            if not target_model:
                raise ValueError(f"Service {llm_service} has no available models")
            
            # æ„å»ºé…ç½®å¯¹è±¡
            provider_config = {
                'provider': service_id,
                'model': target_model.get('name', ''),
                'base_url': service.get('base_url', ''),
                'api_key': service.get('api_key', ''),
                'temperature': target_model.get('temperature', 0.7),
                'max_tokens': target_model.get('max_tokens', 1000),
                'top_p': target_model.get('top_p', 0.9),
                'send_temperature': target_model.get('send_temperature', True),
                'send_top_p': target_model.get('send_top_p', True),
                'send_max_tokens': target_model.get('send_max_tokens', True),
                'custom_params': target_model.get('custom_params', ''),
            }
            
            # Ollamaç‰¹æ®Šå¤„ç†:æ·»åŠ auto_unloadé…ç½®
            if service.get('type') == 'ollama':
                provider_config['auto_unload'] = ollama_auto_unload

            # æ‰§è¡Œæ‰©å†™ï¼ˆå¼‚æ­¥çº¿ç¨‹ + å¯ä¸­æ–­ï¼‰
            request_id = generate_request_id("exp", None, unique_id)
            
            # åˆå¹¶åŸæ–‡ä¸ç”¨æˆ·æç¤ºè¯
            # åˆå¹¶é¡ºåºï¼šè¾“å…¥ç«¯å£(source_text)åœ¨å‰ï¼ŒèŠ‚ç‚¹è¾“å…¥æ¡†(user_prompt)åœ¨å
            combined_text = user_prompt if not source_text else (f"{source_text}\n\n{user_prompt}" if user_prompt else source_text)
            
            # æ£€æŸ¥æ˜¯å¦å…³é—­æ€ç»´é“¾
            model_name = provider_config.get('model')
            disable_thinking_enabled = service.get('disable_thinking', True)
            thinking_extra = build_thinking_suppression(service_id, model_name) if disable_thinking_enabled else None
            model_display = format_model_with_thinking(model_name, bool(thinking_extra))
            
            # è·å–æœåŠ¡æ˜¾ç¤ºåç§°
            service_display_name = service.get('name', service_id)

            # å‡†å¤‡é˜¶æ®µæ—¥å¿—
            log_prepare(TASK_EXPAND, request_id, SOURCE_NODE, service_display_name, model_display, rule_name, {"é•¿åº¦": len(combined_text)})

            # æ£€æŸ¥APIå¯†é’¥å’Œæ¨¡å‹
            api_key = provider_config.get('api_key', '')
            model = provider_config.get('model', '')
            
            if not api_key or not model:
                raise ValueError(f"Please configure API key and model for {llm_service}")

            # æ‰§è¡Œæ‰©å†™ï¼ˆå¼‚æ­¥çº¿ç¨‹ + å¯ä¸­æ–­ï¼‰
            result = self._run_llm_task(
                LLMService.expand_prompt,
                service_id,
                prompt=combined_text,
                request_id=request_id,
                stream_callback=None,
                custom_provider=service_id,
                custom_provider_config=provider_config,
                system_message_override=system_message,
                task_type=TASK_EXPAND,
                source=SOURCE_NODE
            )

            if result and result.get('success'):
                expanded_text = result.get('data', {}).get('expanded', '').strip()
                if not expanded_text:
                    error_msg = 'API returned empty result'
                    log_error(TASK_EXPAND, request_id, error_msg, source=SOURCE_NODE)
                    raise RuntimeError(f"Enhancement failed: {error_msg}")
                # ç»“æœé˜¶æ®µæ—¥å¿—ç”±æœåŠ¡å±‚ç»Ÿä¸€è¾“å‡ºï¼ŒèŠ‚ç‚¹å±‚ä¸å†é‡å¤æ‰“å°
                return (expanded_text,)
            else:
                error_msg = result.get('error', 'Unknown error') if result else 'No result returned'
                # å¦‚æœæ˜¯ä¸­æ–­é”™è¯¯,ç›´æ¥æŠ›å‡ºInterruptProcessingException,ä¸æ‰“å°æ—¥å¿—(ç”±åŸºç±»æ‰“å°)
                if error_msg == "ä»»åŠ¡è¢«ä¸­æ–­":
                    raise InterruptProcessingException()
                log_error(TASK_EXPAND, request_id, error_msg, source=SOURCE_NODE)
                raise RuntimeError(f"Enhancement failed: {error_msg}")

        except InterruptProcessingException:
            # ä¸æ‰“å°æ—¥å¿—,ç”±åŸºç±»ç»Ÿä¸€æ‰“å°
            raise
        except Exception as e:
            error_msg = format_api_error(e, llm_service)
            log_error(TASK_EXPAND, request_id, error_msg, source=SOURCE_NODE)
            raise RuntimeError(f"Enhancement error: {error_msg}")

    # _get_provider_config æ–¹æ³•å·²ç”±åŸºç±» LLMNodeBase æä¾›
    


# èŠ‚ç‚¹æ˜ å°„ï¼Œç”¨äºå‘ComfyUIæ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "PromptExpand": PromptExpand,
}

# èŠ‚ç‚¹æ˜¾ç¤ºåç§°æ˜ å°„
NODE_DISPLAY_NAME_MAPPINGS = {
    "PromptExpand": "âœ¨Prompt Enhance",
}
