import asyncio
import random
import time
import threading
import hashlib
import base64
import os
import tempfile
import shutil
from io import BytesIO
from typing import List, Optional, Union, Tuple

import torch
import numpy as np
from PIL import Image
import imageio

from comfy.model_management import InterruptProcessingException
from ..services.vlm import VisionService
from ..utils.common import format_api_error, format_model_with_thinking, generate_request_id, log_prepare, log_error, TASK_VIDEO_CAPTION, SOURCE_NODE
from ..services.thinking_control import build_thinking_suppression
from .base import VLMNodeBase


class VideoCaptionNode(VLMNodeBase):
    """
    è§†é¢‘åæ¨æç¤ºè¯èŠ‚ç‚¹
    åˆ†æè¾“å…¥è§†é¢‘æˆ–å›¾åƒåºåˆ—å¹¶ç”Ÿæˆæè¿°æ€§æç¤ºè¯
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        # ä»config_managerè·å–ç³»ç»Ÿæç¤ºè¯é…ç½®
        from ..config_manager import config_manager
        system_prompts = config_manager.get_system_prompts()

        # ---è·å–æ‰€æœ‰ video_prompts ä½œä¸ºé€‰é¡¹---
        video_prompts = {}
        if system_prompts and 'video_prompts' in system_prompts:
            video_prompts = system_prompts['video_prompts']

        # æ„å»ºæç¤ºè¯æ¨¡æ¿é€‰é¡¹ï¼ˆæ”¯æŒåˆ†ç±»æ ¼å¼ï¼šç±»åˆ«/è§„åˆ™åç§°ï¼‰
        prompt_template_options = []
        for key, value in video_prompts.items():
            # è¿‡æ»¤æ‰ä¸åœ¨åç«¯æ˜¾ç¤ºçš„è§„åˆ™
            show_in = value.get('showIn', ["frontend", "node"])
            if 'node' not in show_in:
                continue

            name = value.get('name', key)
            category = value.get('category', '')
            # å¦‚æœæœ‰åˆ†ç±»ï¼Œæ˜¾ç¤ºä¸º "ç±»åˆ«/è§„åˆ™åç§°"ï¼Œå¦åˆ™ç›´æ¥æ˜¾ç¤ºè§„åˆ™åç§°
            display_name = f"{category}/{name}" if category else name
            prompt_template_options.append(display_name)

        # å¦‚æœæ²¡æœ‰é€‰é¡¹,æ·»åŠ ä¸€ä¸ªé»˜è®¤é€‰é¡¹
        if not prompt_template_options:
            prompt_template_options = ["é»˜è®¤è§†é¢‘åæ¨æç¤ºè¯"]
        
        # ---åŠ¨æ€è·å–VLMæœåŠ¡/æ¨¡å‹åˆ—è¡¨---
        service_options = cls.get_vlm_service_options()
        default_service = service_options[0] if service_options else "æ™ºè°±"

        return {
            "required": {
                "rule": (prompt_template_options, {"default": prompt_template_options[0] if prompt_template_options else "é»˜è®¤è§†é¢‘åæ¨æç¤ºè¯", "tooltip": "ğŸ’¡Template Config: Settings -> âœ¨Prompt Assistant -> Rule Editor"}),
                "custom_rule": ("BOOLEAN", {"default": False, "label_on": "Enable", "label_off": "Disable", "tooltip": "âš ï¸ Enable to use custom rule content below instead of preset"}),
                "custom_rule_content": ("STRING", {"multiline": True, "default": "", "placeholder": "è¯·è¾“å…¥ä¸´æ—¶è§„åˆ™å†…å®¹,ä»…åœ¨å¯ç”¨'ä¸´æ—¶è§„åˆ™'æ—¶ç”Ÿæ•ˆ", "tooltip": "åœ¨æ­¤è¾“å…¥æ‚¨çš„è‡ªå®šä¹‰è§„åˆ™å†…å®¹; ğŸ’¡è¾“å…¥è§¦å‘è¯[R],å¯ä»¥è®©èŠ‚ç‚¹æ¯æ¬¡éƒ½è¢«æ‰§è¡Œ"}),
                "user_prompt": ("STRING", {"multiline": True, "default": "", "placeholder": "è¾“å…¥é¢å¤–çš„å…·ä½“è¦æ±‚ï¼Œå°†ä¸è§„åˆ™ä¸€èµ·å‘é€ç»™æ¨¡å‹", "tooltip": "è¾“å…¥é¢å¤–çš„å…·ä½“è¦æ±‚ï¼Œå°†ä¸è§„åˆ™ä¸€èµ·å‘é€ç»™æ¨¡å‹; ğŸ’¡è¾“å…¥è§¦å‘è¯[R],å¯ä»¥è®©èŠ‚ç‚¹æ¯æ¬¡éƒ½è¢«æ‰§è¡Œ"}),
                "vlm_service": (service_options, {"default": default_service, "tooltip": "Select VLM service and model"}),
                "sampling_mode": (["Auto (Uniform)", "Manual (Indices)"], {"default": "Auto (Uniform)"}),
                "frame_count": ("INT", {"default": 5, "min": 1, "max": 32, "step": 1, "tooltip": "ğŸ’¡Only for 'Auto' mode. Frame limits: GLM-4Vâ‰¤5, GLM-4.6Vâ‰¤100, Qwen-VLâ‰¤100, Geminiâ‰¤3000, Grokâ‰¤10"}),
                "manual_indices": ("STRING", {"default": "", "placeholder": "Input indices (e.g. 0,10,20) or range (e.g. 0-10)", "tooltip": "ğŸ’¡Only for 'Manual' mode. Supports comma-separated or range. Negative indices allowed."}),
                # Ollama Automatic VRAM Unload
                "ollama_auto_unload": ("BOOLEAN", {"default": True, "label_on": "Enable", "label_off": "Disable", "tooltip": "Auto unload Ollama model after generation"}),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            },
            "optional": {
                "video": ("VIDEO",),
                "image_sequence": ("IMAGE",),
            }
        }

    RETURN_TYPES = ("STRING", "IMAGE")
    RETURN_NAMES = ("prompt_output", "preview_frames")
    FUNCTION = "analyze_video_content"
    CATEGORY = "âœ¨Prompt Assistant"
    OUTPUT_NODE = False
    
    @classmethod
    def IS_CHANGED(cls, video=None, image_sequence=None, rule=None, custom_rule=None, custom_rule_content=None, user_prompt=None, vlm_service=None, sampling_mode=None, frame_count=None, manual_indices=None, ollama_auto_unload=None, unique_id=None):
        """
        åªåœ¨è¾“å…¥å†…å®¹çœŸæ­£å˜åŒ–æ—¶æ‰è§¦å‘é‡æ–°æ‰§è¡Œ
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¼ºåˆ¶åˆ·æ–°ç¬¦å· [R]
        if cls._check_is_changed_bypass(rule, custom_rule_content, user_prompt):
            return float("nan")

        # æå–å®é™…çš„tensoræ•°æ®
        target_input = None
        if video is not None:
            if isinstance(video, dict):
                target_input = video.get('frames') or video.get('video')
            elif hasattr(video, 'video'):
                target_input = video.video
            elif hasattr(video, 'frames'):
                target_input = video.frames
            elif isinstance(video, torch.Tensor):
                target_input = video
        elif image_sequence is not None:
            target_input = image_sequence
        
        # è®¡ç®—å“ˆå¸Œå€¼
        input_data_hash = ""
        if target_input is not None and isinstance(target_input, torch.Tensor):
            try:
                # å–éƒ¨åˆ†å¸§å’Œä¸­å¿ƒåŒºåŸŸè®¡ç®—å“ˆå¸Œ,é¿å…å…¨é‡è®¡ç®—
                if len(target_input.shape) == 4:
                    frames = target_input.shape[0]
                    # å–é¦–å°¾å’Œä¸­é—´å¸§
                    indices = [0, frames // 2, frames - 1] if frames > 2 else range(frames)
                    
                    hash_data = b""
                    for idx in indices:
                        h, w = target_input.shape[1:3]
                        center_h, center_w = h // 2, w // 2
                        size = min(50, h // 4, w // 4)
                        frame_data = target_input[idx,
                                          max(0, center_h - size):min(h, center_h + size),
                                          max(0, center_w - size):min(w, center_w + size),
                                          0].cpu().numpy().tobytes()
                        hash_data += frame_data
                    
                    input_data_hash = hashlib.md5(hash_data).hexdigest()
                else:
                    input_data_hash = "invalid_shape"
            except Exception:
                input_data_hash = "hash_error"

        # ç»„åˆæ‰€æœ‰è¾“å…¥çš„å“ˆå¸Œå€¼
        input_hash = hash((
            input_data_hash,
            rule,
            bool(custom_rule),
            custom_rule_content,
            user_prompt,
            vlm_service,
            sampling_mode,
            frame_count,
            manual_indices,
            bool(ollama_auto_unload)
        ))

        return input_hash
    
    def analyze_video_content(self, rule, custom_rule, custom_rule_content, user_prompt, vlm_service, sampling_mode, frame_count, manual_indices, ollama_auto_unload, video=None, image_sequence=None, unique_id=None):
        """
        åˆ†æè§†é¢‘æˆ–å›¾åƒåºåˆ—å¹¶ç”Ÿæˆæç¤ºè¯(ä½¿ç”¨æŠ½å¸§æ¨¡å¼)
        """
        temp_video_path = None
        try:
            # 1. éªŒè¯è¾“å…¥
            if video is None and image_sequence is None:
                raise ValueError("Video Input or Image Sequence Input is required")
            
            # 2. æå–tensoræ•°æ®
            input_tensor = None
            is_pre_sampled = False
            
            if video is not None:
                # å¤„ç†VideoFromFileå¯¹è±¡(éœ€è¦å…ˆä¿å­˜å†åŠ è½½)
                if hasattr(video, 'save_to') and callable(getattr(video, 'save_to')):
                    try:
                        fd, temp_video_path = tempfile.mkstemp(suffix='.mp4')
                        os.close(fd)
                        video.save_to(temp_video_path)
                        print(f"{self.PROCESS_PREFIX} VideoFromFile saved to temp")
                        #ä»æ–‡ä»¶åŠ è½½ä¸ºtensor,ä¼ å…¥æŠ½å¸§å‚æ•°è¿›è¡Œé¢„é‡‡æ ·ä¼˜åŒ–
                        input_tensor = self._load_video_as_tensor(
                            temp_video_path, 
                            target_count=frame_count if sampling_mode == "Auto (Uniform)" else None,
                            target_indices_str=manual_indices if sampling_mode == "Manual (Indices)" else None
                        )
                        is_pre_sampled = True
                    except Exception as e:
                        if temp_video_path and os.path.exists(temp_video_path):
                            os.unlink(temp_video_path)
                        raise RuntimeError(f"VideoFromFile processing failed: {str(e)}")
                
                # å¤„ç†å­—å…¸æ ¼å¼
                elif isinstance(video, dict):
                    input_tensor = video.get('frames') or video.get('video')
                    if input_tensor is None:
                        for v in video.values():
                            if isinstance(v, torch.Tensor):
                                input_tensor = v
                                break
                
                # å¤„ç†tensor
                elif isinstance(video, torch.Tensor):
                    input_tensor = video
                
                # å°è¯•ç´¢å¼•è®¿é—®
                elif hasattr(video, '__getitem__'):
                    try:
                        first_item = video[0]
                        if isinstance(first_item, torch.Tensor):
                            input_tensor = first_item
                        elif isinstance(first_item, dict):
                            input_tensor = first_item.get('frames') or first_item.get('video')
                    except Exception:
                        pass
                
                if input_tensor is None:
                    raise ValueError(f"Failed to extract tensor from VIDEO input")
            else:
                input_tensor = image_sequence
            
            # 3. å‡†å¤‡æç¤ºè¯
            prompt_template = None
            rule_name = "Custom Rule" if (custom_rule and custom_rule_content) else rule
            
            if custom_rule and custom_rule_content:
                prompt_template = custom_rule_content
            else:
                from ..config_manager import config_manager
                system_prompts = config_manager.get_system_prompts()
                video_prompts = {}
                if system_prompts and 'video_prompts' in system_prompts:
                    video_prompts = system_prompts['video_prompts']

                # æŸ¥æ‰¾æ¨¡æ¿ï¼ˆæŒ‰æ˜¾ç¤ºåç§°åŒ¹é…ï¼‰
                # æ˜¾ç¤ºåç§°æ ¼å¼ï¼šæœ‰åˆ†ç±»æ—¶ä¸º "ç±»åˆ«/è§„åˆ™åç§°"ï¼Œæ— åˆ†ç±»æ—¶ä¸º "è§„åˆ™åç§°"
                for key, value in video_prompts.items():
                    name = value.get('name', key)
                    category = value.get('category', '')
                    # æ„å»ºä¸ä¸‹æ‹‰åˆ—è¡¨ä¸€è‡´çš„æ˜¾ç¤ºåç§°
                    display_name = f"{category}/{name}" if category else name
                    if display_name == rule:
                        prompt_template = value.get('content')
                        break
                
                # å…è®¸ç”¨è§„åˆ™åç§°æˆ–é”®åç›´æ¥åŒ¹é…ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
                if not prompt_template:
                    for key, value in video_prompts.items():
                        if value.get('name') == rule or key == rule:
                            prompt_template = value.get('content')
                            break
                
                if not prompt_template:
                    prompt_template = "è¯·è¯¦ç»†æè¿°è¿™æ®µè§†é¢‘çš„å†…å®¹,åŒ…æ‹¬ä¸»è¦äº‹ä»¶ã€åœºæ™¯å˜åŒ–ã€äººç‰©åŠ¨ä½œå’Œè§†è§‰é£æ ¼ã€‚"
                    rule_name = "Default Rule"

            # æ‹¼æ¥ç”¨æˆ·æç¤ºè¯
            if user_prompt and user_prompt.strip():
                prompt_template = f"{prompt_template}\n\nç”¨æˆ·è¡¥å……è¦æ±‚ï¼š\n{user_prompt}"

            # ---è§£ææœåŠ¡/æ¨¡å‹å­—ç¬¦ä¸²---
            service_id, model_name = self.parse_service_model(vlm_service)
            if not service_id:
                raise ValueError(f"Invalid service selection: {vlm_service}")
            
            # ---è·å–æœåŠ¡é…ç½®---
            from ..config_manager import config_manager
            service = config_manager.get_service(service_id)
            if not service:
                raise ValueError(f"Service config not found: {vlm_service}")
            
            # ---æ„å»ºprovider_config---
            # æŸ¥æ‰¾æŒ‡å®šçš„æ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
            vlm_models = service.get('vlm_models', [])
            target_model = None
            
            if model_name:
                # æŸ¥æ‰¾æŒ‡å®šçš„æ¨¡å‹
                target_model = next((m for m in vlm_models if m.get('name') == model_name), None)
            
            if not target_model:
                # ä½¿ç”¨é»˜è®¤æ¨¡å‹æˆ–ç¬¬ä¸€ä¸ªæ¨¡å‹
                target_model = next((m for m in vlm_models if m.get('is_default')), 
                                    vlm_models[0] if vlm_models else None)
            
            if not target_model:
                raise ValueError(f"Service {vlm_service} has no available models")
            
            # æ„å»ºé…ç½®å¯¹è±¡
            provider_config = {
                'provider': service_id,
                'model': target_model.get('name', ''),
                'base_url': service.get('base_url', ''),
                'api_key': service.get('api_key', ''),
                'temperature': target_model.get('temperature', 0.7),
                'max_tokens': target_model.get('max_tokens', 500),
                'top_p': target_model.get('top_p', 0.9),
                'send_temperature': target_model.get('send_temperature', True),
                'send_top_p': target_model.get('send_top_p', True),
                'send_max_tokens': target_model.get('send_max_tokens', True),
                'custom_params': target_model.get('custom_params', ''),
            }
            
            # Ollamaç‰¹æ®Šå¤„ç†:æ·»åŠ auto_unloadé…ç½®
            if service.get('type') == 'ollama':
                provider_config['auto_unload'] = ollama_auto_unload
            
            model = provider_config.get('model', '')
            
            # 5. æŠ½å¸§å¤„ç†
            request_id = generate_request_id("vcap", None, unique_id)
            # æ£€æŸ¥æ˜¯å¦å…³é—­æ€ç»´é“¾
            disable_thinking_enabled = service.get('disable_thinking', True)
            thinking_extra = build_thinking_suppression(service_id, model) if disable_thinking_enabled else None
            model_display = format_model_with_thinking(model, bool(thinking_extra))
            
            # è·å–æœåŠ¡æ˜¾ç¤ºåç§°
            service_display_name = service.get('name', service_id)
            
            # å‡†å¤‡é˜¶æ®µæ—¥å¿—
            log_prepare(TASK_VIDEO_CAPTION, request_id, SOURCE_NODE, service_display_name, model_display, rule_name, {"æ¨¡å¼": sampling_mode})
            
            # [Debug] è¾“å‡ºæŠ½å¸§å‚æ•°è¯¦æƒ…
            # print(f"{self.PROCESS_PREFIX} [video-caption-debug] è¾“å…¥tensorå½¢çŠ¶:{input_tensor.shape} | is_pre_sampled:{is_pre_sampled}")
            # print(f"{self.PROCESS_PREFIX} [video-caption-debug] sampling_mode:{sampling_mode} | frame_count:{frame_count} | manual_indices:{manual_indices}")
            
            # å‡†å¤‡æŠ½å¸§å‚æ•°
            sampling_kwargs = {}    
            if not is_pre_sampled:
                if sampling_mode == "Auto (Uniform)":
                    sampling_kwargs['target_count'] = frame_count
                elif sampling_mode == "Manual (Indices)":
                    sampling_kwargs['target_indices_str'] = manual_indices
            # ä»tensorä¸­æå–å¸§å¹¶è½¬ä¸ºbase64,åŒæ—¶è·å–é¢„è§ˆtensor
            frames_data, preview_tensor = self._extract_frames_and_tensor(
                input_tensor, 
                **sampling_kwargs
            )
            
            # [Debug] è¾“å‡ºæŠ½å¸§ç»“æœ
            # print(f"{self.PROCESS_PREFIX} [video-caption] æŠ½å¸§å®Œæˆ | å¸§æ•°é‡:{len(frames_data)} | é¢„è§ˆtensor:{preview_tensor.shape}")
            
            # ---æ³¨å…¥å¸§æ•°å…ƒä¿¡æ¯åˆ°æç¤ºè¯---
            # è§£å†³æ¨¡å‹è¯†åˆ«å¸§æ•°ä¸å®é™…å¸§æ•°ä¸ä¸€è‡´çš„é—®é¢˜
            actual_frame_count = len(frames_data)
            frame_info_prefix = f"[é‡è¦æç¤ºï¼šæœ¬æ¬¡å…±æä¾›äº† {actual_frame_count} å¸§å›¾åƒï¼Œè¯·åŠ¡å¿…é€å¸§åˆ†æï¼Œç¡®ä¿è¾“å‡ºçš„æè¿°æ•°é‡ä¸å¸§æ•°ä¸€è‡´ã€‚]\n\n"
            prompt_template = frame_info_prefix + prompt_template
            
            # è°ƒç”¨å¤šå›¾åƒåˆ†æ - ä½¿ç”¨åŸºç±»æ–¹æ³•
            result = self._run_vision_task(
                VisionService.analyze_images,
                service_id,
                images_data=frames_data,
                request_id=request_id,
                prompt_content=prompt_template,
                custom_provider=service_id,
                custom_provider_config=provider_config,
                task_type=TASK_VIDEO_CAPTION,
                source=SOURCE_NODE
            )

            # 6. å¤„ç†ç»“æœ
            if result and result.get('success'):
                description = result.get('data', {}).get('description', '').strip()
                if not description:
                    raise RuntimeError("API returned empty result")
                return (description, preview_tensor)
            else:
                error_msg = result.get('error', 'Unknown error') if result else 'No result returned'
                # å¦‚æœæ˜¯ä¸­æ–­é”™è¯¯,ç›´æ¥æŠ›å‡ºInterruptProcessingException,ä¸æ‰“å°æ—¥å¿—(ç”±åŸºç±»æ‰“å°)
                if error_msg == "ä»»åŠ¡è¢«ä¸­æ–­":
                    raise InterruptProcessingException()
                raise RuntimeError(f"Analysis failed: {error_msg}")

        except InterruptProcessingException:
            # ä¸æ‰“å°æ—¥å¿—,ç”±åŸºç±»ç»Ÿä¸€æ‰“å°
            raise
        except Exception as e:
            error_msg = format_api_error(e, vlm_service)
            log_error(TASK_VIDEO_CAPTION, request_id, error_msg, source=SOURCE_NODE)
            raise RuntimeError(f"Analysis error: {error_msg}")
        finally:
            # æ¸…ç†ä¸´æ—¶è§†é¢‘æ–‡ä»¶
            if temp_video_path and os.path.exists(temp_video_path):
                try:
                    os.unlink(temp_video_path)
                except Exception:
                    pass

    def _uniform_sample(self, l, n):
        """
        ä»åˆ—è¡¨ä¸­å‡åŒ€é‡‡æ · n ä¸ªå…ƒç´  (å‚è€ƒ video_sampling_guide.md)
        ç®—æ³•: å°†åˆ—è¡¨åˆ†æˆ n ä¸ªç­‰é•¿åŒºé—´,ä»æ¯ä¸ªåŒºé—´çš„ä¸­å¿ƒä½ç½®å–æ ·
        """
        if n >= len(l):
            return l
        gap = len(l) / n
        idxs = [int(i * gap + gap / 2) for i in range(n)]
        # ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
        idxs = [min(i, len(l) - 1) for i in idxs]
        return [l[i] for i in idxs]

    def _parse_frame_indices(self, indices_str, total_frames):
        """è§£ææ‰‹åŠ¨è¾“å…¥çš„å¸§ç´¢å¼•å­—ç¬¦ä¸²"""
        indices = set()
        if not indices_str:
            return []
            
        parts = indices_str.split(',')
        for part in parts:
            part = part.strip()
            if not part:
                continue
            try:
                if '-' in part:
                    # Range: 0-10
                    start_str, end_str = part.split('-')
                    start = int(start_str)
                    end = int(end_str)
                    # Handle negative indices
                    if start < 0: start += total_frames
                    if end < 0: end += total_frames
                    
                    start = max(0, min(start, total_frames - 1))
                    end = max(0, min(end, total_frames - 1))
                    
                    if start <= end:
                        indices.update(range(start, end + 1))
                else:
                    # Single index
                    idx = int(part)
                    if idx < 0: idx += total_frames
                    idx = max(0, min(idx, total_frames - 1))
                    indices.add(idx)
            except ValueError:
                print(f"{self.LOG_PREFIX} å¿½ç•¥æ— æ•ˆçš„å¸§ç´¢å¼•æ ¼å¼: {part}")
                
        return sorted(list(indices))

    def _extract_frames_and_tensor(self, tensor, target_count=None, target_indices_str=None):
        """ä»tensorä¸­æå–æŒ‡å®šæ•°é‡æˆ–æŒ‡å®šç´¢å¼•çš„å¸§,è¿”å›(base64åˆ—è¡¨, é¢„è§ˆtensor)"""
        total_frames = tensor.shape[0]
        
        # ç”Ÿæˆæ‰€æœ‰å¸§çš„ç´¢å¼•åˆ—è¡¨
        all_indices = list(range(total_frames))
        
        selected_indices = []
        if target_indices_str is not None:
            # æ‰‹åŠ¨æ¨¡å¼
            selected_indices = self._parse_frame_indices(target_indices_str, total_frames)
            if not selected_indices:
                print(f"{self.PROCESS_PREFIX} âš ï¸ æ‰‹åŠ¨å¸§ç´¢å¼•æ— æ•ˆæˆ–ä¸ºç©º,å›é€€åˆ°è‡ªåŠ¨é‡‡æ ·")
                selected_indices = self._uniform_sample(all_indices, 8) # Default fallback
        elif target_count is not None:
            # è‡ªåŠ¨æ¨¡å¼
            selected_indices = self._uniform_sample(all_indices, target_count)
        else:
            # é»˜è®¤å…¨é‡
            selected_indices = all_indices
        
        # æå–é€‰ä¸­çš„å¸§ [N, H, W, C]
        selected_tensor = tensor[selected_indices]
        
        frames_base64 = []
        # éå†é€‰ä¸­çš„tensorè¿›è¡Œè½¬æ¢
        for i in range(selected_tensor.shape[0]):
            frame_tensor = selected_tensor[i]
            # è½¬ä¸ºnumpy [H, W, C] (0-255)
            frame_np = (frame_tensor.cpu().numpy() * 255).astype(np.uint8)
            # è½¬PIL
            image = Image.fromarray(frame_np)
            # è½¬Base64
            buffer = BytesIO()
            image.save(buffer, format="JPEG", quality=85)
            encoded = base64.b64encode(buffer.getvalue()).decode('utf-8')
            frames_base64.append(f"data:image/jpeg;base64,{encoded}")
            
        return frames_base64, selected_tensor

    def _load_video_as_tensor(self, video_path, target_count=None, target_indices_str=None):
        """
        ä»è§†é¢‘æ–‡ä»¶åŠ è½½ä¸ºtensor
        ä¼˜åŒ–: å¦‚æœæŒ‡å®šäº†target_countæˆ–target_indices_str,åˆ™åªè¯»å–éœ€è¦çš„å¸§ (Sample before Load)
        """
        try:
            import imageio
            # è¯»å–è§†é¢‘
            reader = imageio.get_reader(video_path, 'ffmpeg')
            
            # è·å–æ€»å¸§æ•°
            # å°è¯•ä»å…ƒæ•°æ®è·å–,å¦‚æœå¤±è´¥åˆ™ä½¿ç”¨count_frames (è¾ƒæ…¢)
            try:
                total_frames = reader.count_frames()
            except Exception:
                # å¦‚æœæ— æ³•è·å–å¸§æ•°,å›é€€åˆ°è¯»å–æ‰€æœ‰å¸§
                print(f"{self.PROCESS_PREFIX} æ— æ³•è·å–è§†é¢‘æ€»å¸§æ•°,å°†è¯»å–æ‰€æœ‰å¸§")
                frames = []
                for frame in reader:
                    frame_float = frame.astype(np.float32) / 255.0
                    frames.append(frame_float)
                reader.close()
                if not frames:
                    raise RuntimeError("è§†é¢‘æ–‡ä»¶ä¸­æ²¡æœ‰å¸§")
                frames_array = np.stack(frames, axis=0)
                tensor = torch.from_numpy(frames_array)
                
                # å¦‚æœæœ‰é‡‡æ ·è¦æ±‚,è¿›è¡Œåå¤„ç†é‡‡æ ·
                if target_count is not None or target_indices_str is not None:
                    print(f"{self.PROCESS_PREFIX} è§†é¢‘åŠ è½½å›é€€: è¯»å–å…¨é‡å¸§åè¿›è¡Œé‡‡æ ·")
                    _, tensor = self._extract_frames_and_tensor(tensor, target_count, target_indices_str)
                    
                return tensor

            # è®¡ç®—éœ€è¦è¯»å–çš„å¸§ç´¢å¼•
            indices_to_read = list(range(total_frames))
            
            if target_indices_str is not None:
                indices_to_read = self._parse_frame_indices(target_indices_str, total_frames)
                print(f"{self.PROCESS_PREFIX} è§†é¢‘åŠ è½½ä¼˜åŒ–(æ‰‹åŠ¨): ä» {total_frames} å¸§ä¸­æå– {len(indices_to_read)} å¸§")
            elif target_count and total_frames > target_count:
                indices_to_read = self._uniform_sample(indices_to_read, target_count)
                print(f"{self.PROCESS_PREFIX} è§†é¢‘åŠ è½½ä¼˜åŒ–(è‡ªåŠ¨): ä» {total_frames} å¸§ä¸­é‡‡æ · {len(indices_to_read)} å¸§")
            
            # ä¼˜åŒ–è¯»å–: åªè¯»å–éœ€è¦çš„å¸§
            frames = []
           
            # imageioçš„get_data(index)æ”¯æŒéšæœºè®¿é—®
            for idx in indices_to_read:
                try:
                    frame = reader.get_data(idx)
                    frame_float = frame.astype(np.float32) / 255.0
                    frames.append(frame_float)
                except IndexError:
                    break
            
            reader.close()
            
            if not frames:
                raise RuntimeError("æœªèƒ½è¯»å–åˆ°æœ‰æ•ˆå¸§")
            
            # è½¬æ¢ä¸ºtorch tensor [N, H, W, C]
            frames_array = np.stack(frames, axis=0)
            tensor = torch.from_numpy(frames_array)
            
            return tensor
        except Exception as e:
            raise RuntimeError(f"è§†é¢‘æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")

# èŠ‚ç‚¹æ˜ å°„ï¼Œç”¨äºComfyUIæ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "VideoCaptionNode": VideoCaptionNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoCaptionNode": "âœ¨Video Caption",
}
