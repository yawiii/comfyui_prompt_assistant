import asyncio
import random
import time
import threading
import hashlib
import base64
from io import BytesIO
import os
import json

import torch
import numpy as np
from PIL import Image
from comfy.model_management import InterruptProcessingException

from ..services.vlm import VisionService
from ..utils.common import format_api_error, format_model_with_thinking, generate_request_id, log_prepare, log_error, TASK_IMAGE_CAPTION, SOURCE_NODE
from ..services.thinking_control import build_thinking_suppression
from .base import VLMNodeBase


class ImageCaptionNode(VLMNodeBase):
    """
    å›¾åƒåæ¨æç¤ºè¯èŠ‚ç‚¹
    åˆ†æè¾“å…¥å›¾åƒå¹¶ç”Ÿæˆæè¿°æ€§æç¤ºè¯
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        # ä»config_managerè·å–ç³»ç»Ÿæç¤ºè¯é…ç½®
        from ..config_manager import config_manager
        system_prompts = config_manager.get_system_prompts()

        # è·å–æ‰€æœ‰vision_promptsä½œä¸ºé€‰é¡¹
        vision_prompts = {}
        if system_prompts and 'vision_prompts' in system_prompts:
            vision_prompts = system_prompts['vision_prompts']

        # æ„å»ºæç¤ºè¯æ¨¡æ¿é€‰é¡¹ï¼ˆæ”¯æŒåˆ†ç±»æ ¼å¼ï¼šç±»åˆ«/è§„åˆ™åç§°ï¼‰
        prompt_template_options = []
        for key, value in vision_prompts.items():
            # è¿‡æ»¤æ‰ä¸åœ¨åç«¯æ˜¾ç¤ºçš„è§„åˆ™
            show_in = value.get('showIn', ["frontend", "node"])
            if 'node' not in show_in:
                continue

            name = value.get('name', key)
            category = value.get('category', '')
            # å¦‚æœæœ‰åˆ†ç±»ï¼Œæ˜¾ç¤ºä¸º "ç±»åˆ«/è§„åˆ™åç§°"ï¼Œå¦åˆ™ç›´æ¥æ˜¾ç¤ºè§„åˆ™åç§°
            display_name = f"{category}/{name}" if category else name
            prompt_template_options.append(display_name)

        # å¦‚æœæ²¡æœ‰é€‰é¡¹ï¼Œæ·»åŠ ä¸€ä¸ªé»˜è®¤é€‰é¡¹
        if not prompt_template_options:
            prompt_template_options = ["é»˜è®¤ä¸­æ–‡åæ¨æç¤ºè¯"]
        
        # ---åŠ¨æ€è·å–VLMæœåŠ¡/æ¨¡å‹åˆ—è¡¨---
        service_options = cls.get_vlm_service_options()
        default_service = service_options[0] if service_options else "æ™ºè°±"

        return {
            "required": {
                "image": ("IMAGE",),
                "rule": (prompt_template_options, {"default": prompt_template_options[0] if prompt_template_options else "é»˜è®¤ä¸­æ–‡åæ¨æç¤ºè¯", "tooltip": "Choose a preset rule for image captioning"}),
                "custom_rule": ("BOOLEAN", {"default": False, "label_on": "Enable", "label_off": "Disable", "tooltip": "Enable to use custom rule content below"}),
                "custom_rule_content": ("STRING", {"multiline": True, "default": "", "placeholder": "åœ¨æ­¤è¾“å…¥ä¸´æ—¶è§„åˆ™ï¼Œä»…åœ¨å¯ç”¨'ä¸´æ—¶è§„åˆ™'æ—¶ç”Ÿæ•ˆ", "tooltip": "åœ¨æ­¤è¾“å…¥æ‚¨çš„è‡ªå®šä¹‰è§„åˆ™å†…å®¹; ğŸ’¡è¾“å…¥è§¦å‘è¯[R],å¯ä»¥è®©èŠ‚ç‚¹æ¯æ¬¡éƒ½è¢«æ‰§è¡Œ"}),
                "user_prompt": ("STRING", {"multiline": True, "default": "", "placeholder": "è¾“å…¥é¢å¤–çš„å…·ä½“è¦æ±‚ï¼Œå°†ä¸è§„åˆ™ä¸€èµ·å‘é€ç»™æ¨¡å‹", "tooltip": "è¾“å…¥é¢å¤–çš„å…·ä½“è¦æ±‚ï¼Œå°†ä¸è§„åˆ™ä¸€èµ·å‘é€ç»™æ¨¡å‹; ğŸ’¡è¾“å…¥è§¦å‘è¯[R],å¯ä»¥è®©èŠ‚ç‚¹æ¯æ¬¡éƒ½è¢«æ‰§è¡Œ"}),
                "vlm_service": (service_options, {"default": default_service, "tooltip": "Select VLM service and model"}),
                # Ollama Automatic VRAM Unload
                "ollama_auto_unload": ("BOOLEAN", {"default": True, "label_on": "Enable", "label_off": "Disable", "tooltip": "Auto unload Ollama model after generation"}),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("prompt_output", "prompt_list")
    OUTPUT_IS_LIST = (False, True)
    FUNCTION = "analyze_image"
    CATEGORY = "âœ¨Prompt Assistant"
    OUTPUT_NODE = False
    
    @classmethod
    def IS_CHANGED(cls, image=None, rule=None, custom_rule=None, custom_rule_content=None, user_prompt=None, vlm_service=None, ollama_auto_unload=None, unique_id=None):
        """
        åªåœ¨è¾“å…¥å†…å®¹çœŸæ­£å˜åŒ–æ—¶æ‰è§¦å‘é‡æ–°æ‰§è¡Œ
        ä½¿ç”¨è¾“å…¥å‚æ•°çš„å“ˆå¸Œå€¼ä½œä¸ºåˆ¤æ–­ä¾æ®
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¼ºåˆ¶åˆ·æ–°ç¬¦å· [R]
        if cls._check_is_changed_bypass(rule, custom_rule_content, user_prompt):
            return float("nan")

        # å¯¼å…¥å›¾åƒå“ˆå¸Œå·¥å…·å‡½æ•°
        from ..utils.image import compute_image_hash
        
        # è®¡ç®—å›¾åƒçš„å“ˆå¸Œå€¼
        img_hash = compute_image_hash(image)

        # ç»„åˆæ‰€æœ‰è¾“å…¥çš„å“ˆå¸Œå€¼
        input_hash = hash((
            img_hash,
            rule,
            bool(custom_rule),
            custom_rule_content,
            user_prompt,
            vlm_service,
            bool(ollama_auto_unload)
        ))

        return input_hash
    
    def _analyze_single_image(self, image_data, prompt_template, rule_name, service_id, service, provider_config, unique_id, frame_index=None):
        """
        åˆ†æå•å¼ å›¾åƒï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰
        
        Args:
            image_data: base64ç¼–ç çš„å›¾åƒæ•°æ®
            prompt_template: æç¤ºè¯æ¨¡æ¿
            rule_name: è§„åˆ™åç§°
            service_id: æœåŠ¡ID
            service: æœåŠ¡é…ç½®
            provider_config: Provideré…ç½®
            unique_id: èŠ‚ç‚¹å”¯ä¸€ID
            frame_index: æ‰¹æ¬¡ä¸­çš„å¸§ç´¢å¼•ï¼ˆç”¨äºæ—¥å¿—æ ‡è¯†ï¼‰
        
        Returns:
            str: å›¾åƒæè¿°ç»“æœ
        """
        # åˆ›å»ºè¯·æ±‚IDï¼ˆåŒ…å«å¸§ç´¢å¼•ä¿¡æ¯ï¼‰
        frame_suffix = f"_f{frame_index}" if frame_index is not None else ""
        request_id = generate_request_id("icap", None, unique_id) + frame_suffix
        
        # æ£€æŸ¥æ˜¯å¦å…³é—­æ€ç»´é“¾
        model_full_name = provider_config.get('model')
        disable_thinking_enabled = service.get('disable_thinking', True)
        thinking_extra = build_thinking_suppression(service_id, model_full_name) if disable_thinking_enabled else None
        model_display = format_model_with_thinking(model_full_name, bool(thinking_extra))
        
        # è·å–æœåŠ¡æ˜¾ç¤ºåç§°
        service_display_name = service.get('name', service_id)
        
        # å‡†å¤‡é˜¶æ®µæ—¥å¿—ï¼ˆåŒ…å«å¸§ä¿¡æ¯ï¼‰
        frame_info = f" [å¸§ {frame_index + 1}]" if frame_index is not None else ""
        log_prepare(TASK_IMAGE_CAPTION, request_id, SOURCE_NODE, service_display_name, model_display, rule_name + frame_info)

        # æ‰§è¡Œå›¾åƒåˆ†æ
        result = self._run_vision_task(
            VisionService.analyze_image,
            service_id,
            image_data=image_data,
            request_id=request_id,
            stream_callback=None,
            prompt_content=prompt_template,
            custom_provider=service_id,
            custom_provider_config=provider_config,
            task_type=TASK_IMAGE_CAPTION,
            source=SOURCE_NODE
        )

        if result and result.get('success'):
            description = result.get('data', {}).get('description', '').strip()
            if not description:
                error_msg = 'APIè¿”å›ç»“æœä¸ºç©ºï¼Œè¯·æ£€æŸ¥APIå¯†é’¥ã€æ¨¡å‹é…ç½®æˆ–ç½‘ç»œè¿æ¥'
                log_error(TASK_IMAGE_CAPTION, request_id, error_msg, source=SOURCE_NODE)
                raise RuntimeError(f"åˆ†æå¤±è´¥: {error_msg}")
            return description
        else:
            error_msg = result.get('error', 'åˆ†æå¤±è´¥ï¼ŒæœªçŸ¥é”™è¯¯') if result else 'åˆ†ææœåŠ¡æœªè¿”å›ç»“æœ'
            if error_msg == "ä»»åŠ¡è¢«ä¸­æ–­":
                raise InterruptProcessingException()
            log_error(TASK_IMAGE_CAPTION, request_id, error_msg, source=SOURCE_NODE)
            raise RuntimeError(f"åˆ†æå¤±è´¥: {error_msg}")

    def analyze_image(self, image, rule, custom_rule, custom_rule_content, user_prompt, vlm_service, ollama_auto_unload, unique_id=None):
        """
        åˆ†æå›¾åƒå¹¶ç”Ÿæˆæç¤ºè¯ï¼ˆæ”¯æŒ batch éå†ï¼‰

        Args:
            image: è¾“å…¥çš„å›¾åƒæ•°æ®ï¼Œæ”¯æŒå•å¼ æˆ– batch
            rule: é€‰æ‹©çš„æç¤ºè¯æ¨¡æ¿
            custom_rule: æ˜¯å¦å¯ç”¨ä¸´æ—¶è§„åˆ™
            custom_rule_content: ä¸´æ—¶è§„åˆ™çš„å†…å®¹
            user_prompt: ç”¨æˆ·è¡¥å……çš„æç¤ºè¯
            vlm_service: é€‰æ‹©çš„è§†è§‰æœåŠ¡
            ollama_auto_unload: Ollama è‡ªåŠ¨å¸è½½å¼€å…³
            unique_id: èŠ‚ç‚¹å”¯ä¸€ID

        Returns:
            tuple: åˆ†æç»“æœï¼Œbatch è¾“å…¥æ—¶ç»“æœç”¨æ¢è¡Œç¬¦åˆ†éš”
        """
        request_id = None  # åˆå§‹åŒ–ï¼Œç”¨äºå¼‚å¸¸å¤„ç†
        
        try:
            # æ£€æŸ¥è¾“å…¥
            if image is None:
                raise ValueError("è¾“å…¥å›¾åƒä¸èƒ½ä¸ºç©º")

            # ---å‡†å¤‡æç¤ºè¯æ¨¡æ¿---
            prompt_template = None
            rule_name = "Custom Rule" if (custom_rule and custom_rule_content) else rule
            
            if custom_rule and custom_rule_content:
                prompt_template = custom_rule_content
            else:
                from ..config_manager import config_manager
                system_prompts = config_manager.get_system_prompts()

                vision_prompts = {}
                if system_prompts and 'vision_prompts' in system_prompts:
                    vision_prompts = system_prompts['vision_prompts']

                # æŒ‰æ˜¾ç¤ºåç§°åŒ¹é…
                template_found = False
                for key, value in vision_prompts.items():
                    name = value.get('name', key)
                    category = value.get('category', '')
                    display_name = f"{category}/{name}" if category else name
                    if display_name == rule:
                        prompt_template = value.get('content')
                        template_found = True
                        break

                if not template_found:
                    # å…¼å®¹æ—§æ ¼å¼
                    for key, value in vision_prompts.items():
                        if value.get('name') == rule or key == rule:
                            prompt_template = value.get('content')
                            template_found = True
                            break

                if not template_found or not prompt_template:
                    prompt_template = "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»ä½“ã€åœºæ™¯ã€é£æ ¼ã€è‰²å½©ç­‰è¦ç´ ã€‚"
                    rule_name = "Default Rule"

            # æ‹¼æ¥ç”¨æˆ·æç¤ºè¯
            if user_prompt and user_prompt.strip():
                prompt_template = f"{prompt_template}\n\nç”¨æˆ·è¡¥å……è¦æ±‚ï¼š\n{user_prompt}"

            # ---è§£ææœåŠ¡é…ç½®---
            service_id, model_name = self.parse_service_model(vlm_service)
            if not service_id:
                raise ValueError(f"Invalid service selection: {vlm_service}")
            
            from ..config_manager import config_manager
            service = config_manager.get_service(service_id)
            if not service:
                raise ValueError(f"Service config not found: {vlm_service}")
            
            # æ„å»º provider_config
            vlm_models = service.get('vlm_models', [])
            target_model = None
            
            if model_name:
                target_model = next((m for m in vlm_models if m.get('name') == model_name), None)
            
            if not target_model:
                target_model = next((m for m in vlm_models if m.get('is_default')), 
                                    vlm_models[0] if vlm_models else None)
            
            if not target_model:
                raise ValueError(f"Service {vlm_service} has no available models")
            
            provider_config = {
                'provider': service_id,
                'model': target_model.get('name', ''),
                'base_url': service.get('base_url', ''),
                'api_key': service.get('api_key', ''),
                'temperature': target_model.get('temperature', 0.7),
                'max_tokens': target_model.get('max_tokens', 500),
                'top_p': target_model.get('top_p', 0.9),
                'send_temperature': target_model.get('send_temperature', True),
                'send_top_p': target_model.get('send_top_p', True),
                'send_max_tokens': target_model.get('send_max_tokens', True),
                'custom_params': target_model.get('custom_params', ''),
            }
            
            if service.get('type') == 'ollama':
                provider_config['auto_unload'] = ollama_auto_unload

            # ---å¤„ç† batch è¾“å…¥---
            # æ£€æŸ¥æ˜¯å¦ä¸º 4D tensorï¼ˆbatch æ ¼å¼ï¼‰
            if len(image.shape) == 4 and image.shape[0] > 1:
                # Batch æ¨¡å¼ï¼šé€å¸§å¤„ç†
                batch_size = image.shape[0]
                results = []
                
                for i in range(batch_size):
                    # æå–å•å¸§å¹¶è½¬æ¢ä¸º base64
                    single_frame = image[i:i+1]  # ä¿æŒ 4D å½¢çŠ¶ [1, H, W, C]
                    image_data = self._image_to_base64(single_frame)
                    
                    # åˆ†æå•å¼ å›¾åƒ
                    description = self._analyze_single_image(
                        image_data=image_data,
                        prompt_template=prompt_template,
                        rule_name=rule_name,
                        service_id=service_id,
                        service=service,
                        provider_config=provider_config,
                        unique_id=unique_id,
                        frame_index=i
                    )
                    results.append(description)
                
                # è¾“å‡ºï¼šåˆå¹¶ç»“æœ + åˆ—è¡¨
                combined_result = "\n---\n".join(results)
                return (combined_result, results)
            else:
                # å•å¼ æ¨¡å¼
                image_data = self._image_to_base64(image)
                description = self._analyze_single_image(
                    image_data=image_data,
                    prompt_template=prompt_template,
                    rule_name=rule_name,
                    service_id=service_id,
                    service=service,
                    provider_config=provider_config,
                    unique_id=unique_id,
                    frame_index=None
                )
                return (description, [description])

        except InterruptProcessingException:
            raise
        except Exception as e:
            error_msg = format_api_error(e, vlm_service)
            if request_id:
                log_error(TASK_IMAGE_CAPTION, request_id, error_msg, source=SOURCE_NODE)
            raise RuntimeError(f"åˆ†æå¼‚å¸¸: {error_msg}")


# èŠ‚ç‚¹æ˜ å°„ï¼Œç”¨äºå‘ComfyUIæ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "ImageCaptionNode": ImageCaptionNode,
}

# èŠ‚ç‚¹æ˜¾ç¤ºåç§°æ˜ å°„
NODE_DISPLAY_NAME_MAPPINGS = {
    "ImageCaptionNode": "âœ¨å›¾åƒåæ¨æç¤ºè¯",
} 
