import asyncio
import random
import time
import threading
import hashlib
import base64
from io import BytesIO
import os
import json

import torch
import numpy as np
from PIL import Image
from comfy.model_management import InterruptProcessingException

from ..services.vlm import VisionService
from ..utils.common import format_api_error, log_prepare, log_error, SOURCE_NODE, generate_request_id
from .base import VLMNodeBase


class KontextPresetNode(VLMNodeBase):
    """
    Kontexté¢„è®¾åŠ©æ‰‹èŠ‚ç‚¹
    ä½¿ç”¨Kontexté¢„è®¾åˆ†æå›¾åƒå¹¶ç”Ÿæˆåˆ›æ„è½¬æ¢æŒ‡ä»¤
    """
    
    # ç¼“å­˜é…ç½®æ•°æ®ï¼Œé¿å…é‡å¤ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–
    _kontext_config = None
    
    @classmethod
    def _load_kontext_config(cls):
        """åŠ è½½Kontexté…ç½®ï¼Œä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è¯»å–æ–‡ä»¶"""
        if cls._kontext_config is None:
            try:
                from ..config_manager import config_manager
                # ä½¿ç”¨ config_manager çš„ kontext_presets_path (æŒ‡å‘ rules ç›®å½•)
                kontext_presets_path = config_manager.kontext_presets_path
                
                if os.path.exists(kontext_presets_path):
                    with open(kontext_presets_path, "r", encoding="utf-8") as f:
                        cls._kontext_config = json.load(f)
                else:
                    cls._kontext_config = {}
            except Exception as e:
                print(f"{cls.LOG_PREFIX} åŠ è½½Kontexté…ç½®å¤±è´¥: {str(e)}")
                cls._kontext_config = {}
        return cls._kontext_config

    
    @classmethod
    def INPUT_TYPES(cls):
        # è·å–kontext_presets
        kontext_presets = {}
        config_data = cls._load_kontext_config()
        if 'kontext_presets' in config_data:
            kontext_presets = config_data['kontext_presets']

        # æ„å»ºæç¤ºè¯æ¨¡æ¿é€‰é¡¹
        prompt_template_options = []
        for key, value in kontext_presets.items():
            name = value.get('name', key)
            prompt_template_options.append(name)

        # å¦‚æœæ²¡æœ‰é€‰é¡¹ï¼Œæ·»åŠ ä¸€ä¸ªé»˜è®¤é€‰é¡¹
        if not prompt_template_options:
            prompt_template_options = ["æƒ…å¢ƒæ·±åº¦èåˆ"]
        
        # ---åŠ¨æ€è·å–VLMæœåŠ¡/æ¨¡å‹åˆ—è¡¨---
        service_options = cls.get_vlm_service_options()
        default_service = service_options[0] if service_options else "æ™ºè°±"

        return {
            "required": {
                "image": ("IMAGE",),
                "kontext_preset": (prompt_template_options, {"default": prompt_template_options[0] if prompt_template_options else "æƒ…å¢ƒæ·±åº¦èåˆ"}),
                "user_prompt": ("STRING", {"multiline": True, "default": "", "placeholder": "è¾“å…¥é¢å¤–çš„å…·ä½“è¦æ±‚,å°†ä¸é¢„è®¾ä¸€èµ·å‘é€ç»™æ¨¡å‹", "tooltip": "è¾“å…¥é¢å¤–çš„å…·ä½“è¦æ±‚,å°†ä¸é¢„è®¾ä¸€èµ·å‘é€ç»™æ¨¡å‹; ğŸ’¡è¾“å…¥è§¦å‘è¯[R],å¯ä»¥è®©èŠ‚ç‚¹æ¯æ¬¡éƒ½è¢«æ‰§è¡Œ"}),
                "vlm_service": (service_options, {"default": default_service, "tooltip": "Select VLM service and model"}),
                # Ollama Automatic VRAM Unload
                "ollama_auto_unload": ("BOOLEAN", {"default": True, "label_on": "Enable", "label_off": "Disable", "tooltip": "Auto unload Ollama model after generation"}),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("creative_instruction",)
    FUNCTION = "analyze_image"
    CATEGORY = "âœ¨Prompt Assistant"
    OUTPUT_NODE = False
    
    @classmethod
    def IS_CHANGED(cls, image=None, kontext_preset=None, user_prompt=None, vlm_service=None, ollama_auto_unload=None):
        """
        åªåœ¨è¾“å…¥å†…å®¹çœŸæ­£å˜åŒ–æ—¶æ‰è§¦å‘é‡æ–°æ‰§è¡Œ
        ä½¿ç”¨è¾“å…¥å‚æ•°çš„å“ˆå¸Œå€¼ä½œä¸ºåˆ¤æ–­ä¾æ®
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¼ºåˆ¶åˆ·æ–°ç¬¦å· [R]
        if cls._check_is_changed_bypass(kontext_preset, user_prompt):
            return float("nan")

        # å¯¼å…¥å›¾åƒå“ˆå¸Œå·¥å…·å‡½æ•°
        from ..utils.image import compute_image_hash
        
        # è®¡ç®—å›¾åƒçš„å“ˆå¸Œå€¼
        img_hash = compute_image_hash(image)

        # ç»„åˆæ‰€æœ‰è¾“å…¥çš„å“ˆå¸Œå€¼
        input_hash = hash((
            img_hash,
            kontext_preset,
            user_prompt,
            vlm_service,
            bool(ollama_auto_unload)
        ))

        return input_hash
    
    def analyze_image(self, image, kontext_preset, user_prompt, vlm_service, ollama_auto_unload):
        """
        ä½¿ç”¨Kontexté¢„è®¾åˆ†æå›¾åƒå¹¶ç”Ÿæˆåˆ›æ„è½¬æ¢æŒ‡ä»¤

        Args:
            image: è¾“å…¥çš„å›¾åƒæ•°æ®
            kontext_preset: é€‰æ‹©çš„Kontexté¢„è®¾
            user_prompt: ç”¨æˆ·è¡¥å……çš„æç¤ºè¯
            vlm_service: é€‰æ‹©çš„è§†è§‰æœåŠ¡

        Returns:
            tuple: åˆ†æç»“æœ
        """
        try:
            # æ£€æŸ¥è¾“å…¥
            if image is None:
                raise ValueError("è¾“å…¥å›¾åƒä¸èƒ½ä¸ºç©º")

            # å°†å›¾åƒè½¬æ¢ä¸ºbase64ç¼–ç 
            image_data = self._image_to_base64(image)

            # è·å–kontexté…ç½®
            config_data = self.__class__._load_kontext_config()
            kontext_prefix = config_data.get('kontext_prefix', "")
            kontext_suffix = config_data.get('kontext_suffix', "")
            kontext_presets = config_data.get('kontext_presets', {})
            
            # è·å–æç¤ºè¯æ¨¡æ¿å†…å®¹
            prompt_template = None

            # æŸ¥æ‰¾é€‰å®šçš„æç¤ºè¯æ¨¡æ¿
            preset_name = kontext_preset
            template_found = False
            for key, value in kontext_presets.items():
                if value.get('name') == kontext_preset:
                    prompt_template = value.get('content')
                    template_found = True
                    break

            if not template_found:
                # å°è¯•ç›´æ¥åŒ¹é…é”®å
                for key, value in kontext_presets.items():
                    if key == kontext_preset or key == f"kontext_{kontext_preset}":
                        prompt_template = value.get('content')
                        template_found = True
                        break

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not prompt_template:
                prompt_template = "Transform the image into a detailed pencil sketch with fine lines and careful shading."
                preset_name = "é»˜è®¤é¢„è®¾"

            # æ„å»ºæœ€ç»ˆæç¤ºè¯ï¼Œæ·»åŠ å‰ç¼€å’Œåç¼€
            final_prompt = prompt_template
            if kontext_prefix and kontext_suffix:
                final_prompt = f"{kontext_prefix}\n\nThe Brief: {prompt_template}\n\n{kontext_suffix}"
            
            # æ‹¼æ¥ç”¨æˆ·æç¤ºè¯
            if user_prompt and user_prompt.strip():
                final_prompt = f"{final_prompt}\n\nç”¨æˆ·è¡¥å……è¦æ±‚ï¼š\n{user_prompt}"

            # ---è§£ææœåŠ¡/æ¨¡å‹å­—ç¬¦ä¸²---
            service_id, model_name = self.parse_service_model(vlm_service)
            if not service_id:
                raise ValueError(f"Invalid service selection: {vlm_service}")
            
            # ---è·å–æœåŠ¡é…ç½®---
            from ..config_manager import config_manager
            service = config_manager.get_service(service_id)
            if not service:
                raise ValueError(f"Service config not found: {vlm_service}")
            
            # ---æ„å»ºprovider_config---
            # æŸ¥æ‰¾æŒ‡å®šçš„æ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
            vlm_models = service.get('vlm_models', [])
            target_model = None
            
            if model_name:
                # æŸ¥æ‰¾æŒ‡å®šçš„æ¨¡å‹
                target_model = next((m for m in vlm_models if m.get('name') == model_name), None)
            
            if not target_model:
                # ä½¿ç”¨é»˜è®¤æ¨¡å‹æˆ–ç¬¬ä¸€ä¸ªæ¨¡å‹
                target_model = next((m for m in vlm_models if m.get('is_default')), 
                                    vlm_models[0] if vlm_models else None)
            
            if not target_model:
                raise ValueError(f"Service {vlm_service} has no available models")
            
            # æ„å»ºé…ç½®å¯¹è±¡
            provider_config = {
                'provider': service_id,
                'model': target_model.get('name', ''),
                'base_url': service.get('base_url', ''),
                'api_key': service.get('api_key', ''),
                'temperature': target_model.get('temperature', 0.7),
                'max_tokens': target_model.get('max_tokens', 500),
                'top_p': target_model.get('top_p', 0.9),
                'send_temperature': target_model.get('send_temperature', True),
                'send_top_p': target_model.get('send_top_p', True),
                'send_max_tokens': target_model.get('send_max_tokens', True),
                'custom_params': target_model.get('custom_params', ''),
            }
            
            # Ollamaç‰¹æ®Šå¤„ç†:æ·»åŠ auto_unloadé…ç½®
            if service.get('type') == 'ollama':
                provider_config['auto_unload'] = ollama_auto_unload

            # åˆ›å»ºè¯·æ±‚ID
            request_id = f"kontext_preset_{int(time.time())}_{random.randint(1000, 9999)}"
            
            # è·å–æœåŠ¡æ˜¾ç¤ºåç§°
            service_display_name = service.get('name', service_id)
            
            # å‡†å¤‡é˜¶æ®µæ—¥å¿—
            log_prepare("Kontexté¢„è®¾", request_id, SOURCE_NODE, service_display_name, provider_config.get('model'), preset_name)

            # æ‰§è¡Œå›¾åƒåˆ†æ
            result = self._run_vision_task(
                VisionService.analyze_image,
                service_id,
                image_data=image_data,
                request_id=request_id,
                stream_callback=None,
                prompt_content=final_prompt,
                custom_provider=service_id,
                custom_provider_config=provider_config,
                task_type="Kontexté¢„è®¾",
                source=SOURCE_NODE
            )

            if result and result.get('success'):
                description = result.get('data', {}).get('description', '').strip()
                if not description:
                    error_msg = 'API returned empty result'
                    log_error("Kontexté¢„è®¾", request_id, error_msg, source=SOURCE_NODE)
                    raise RuntimeError(f"Analysis failed: {error_msg}")

                # æœåŠ¡å±‚å·²ç»æ‰“å°äº†å®Œæˆæ—¥å¿—ï¼Œè¿™é‡Œä¸å†é‡å¤
                return (description,)
            else:
                error_msg = result.get('error', 'Unknown error') if result else 'No result returned'
                # å¦‚æœæ˜¯ä¸­æ–­é”™è¯¯,ç›´æ¥æ‰“å°æ—¥å¿—å¹¶æŠ›å‡ºInterruptProcessingException
                if error_msg == "ä»»åŠ¡è¢«ä¸­æ–­":
                    print(f"{self.LOG_PREFIX} â›”ï¸Task cancelled by user | RequestID:{request_id}")
                    raise InterruptProcessingException()
                log_error("Kontexté¢„è®¾", request_id, error_msg, source=SOURCE_NODE)
                raise RuntimeError(f"Analysis failed: {error_msg}")

        except InterruptProcessingException:
            print(f"{self.LOG_PREFIX} â›”ï¸Task cancelled by user | RequestID:{request_id}")
            raise
        except Exception as e:
            error_msg = format_api_error(e, vlm_service)
            log_error("Kontexté¢„è®¾", request_id, error_msg, source=SOURCE_NODE)
            raise RuntimeError(f"Analysis error: {error_msg}")


# èŠ‚ç‚¹æ˜ å°„ï¼Œç”¨äºå‘ComfyUIæ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "KontextPresetNode": KontextPresetNode,
}

# èŠ‚ç‚¹æ˜¾ç¤ºåç§°æ˜ å°„
NODE_DISPLAY_NAME_MAPPINGS = {
    "KontextPresetNode": "âœ¨Kontext Preset",
} 
