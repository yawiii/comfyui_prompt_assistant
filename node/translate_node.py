import asyncio
import random
import re
import time
import threading
import hashlib

import torch
from comfy.model_management import InterruptProcessingException

from ..services.llm import LLMService
from ..services.baidu import BaiduTranslateService
from ..utils.common import format_api_error, format_model_with_thinking, generate_request_id, log_prepare, log_error, TASK_TRANSLATE, SOURCE_NODE
from ..services.thinking_control import build_thinking_suppression
from .base import LLMNodeBase


class PromptTranslate(LLMNodeBase):
    """
    æç¤ºè¯ç¿»è¯‘èŠ‚ç‚¹
    è‡ªåŠ¨è¯†åˆ«è¾“å…¥è¯­è¨€å¹¶ç¿»è¯‘æˆç›®æ ‡è¯­è¨€ï¼Œæ”¯æŒå¤šç§ç¿»è¯‘æœåŠ¡
    """

    @classmethod
    def INPUT_TYPES(cls):
        # ---åŠ¨æ€è·å–ç¿»è¯‘æœåŠ¡/æ¨¡å‹åˆ—è¡¨(åŒ…å«ç¡¬ç¼–ç çš„ç™¾åº¦ç¿»è¯‘)---
        service_options = cls.get_translate_service_options()
        default_service = service_options[0] if service_options else "ç™¾åº¦ç¿»è¯‘"
        
        return {
            "required": {
                "source_text": ("STRING", {"forceInput": True, "default": "", "multiline": True, "placeholder": "Input text to translate...", "tooltip": "éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬; ğŸ’¡è¾“å…¥è§¦å‘è¯[R],å¯ä»¥è®©èŠ‚ç‚¹æ¯æ¬¡éƒ½è¢«æ‰§è¡Œ"}),
                "target_language": (["English", "Chinese"], {"default": "English"}),
                "translate_service": (service_options, {"default": default_service, "tooltip": "Select translation service and model"}),
                # Ollama Automatic VRAM Unload
                "ollama_auto_unload": ("BOOLEAN", {"default": True, "label_on": "Enable", "label_off": "Disable", "tooltip": "Auto unload Ollama model after generation"}),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("translated_text",)
    FUNCTION = "translate"
    CATEGORY = "âœ¨Prompt Assistant"
    OUTPUT_NODE = False
    
    @classmethod
    def IS_CHANGED(cls, source_text=None, target_language=None, translate_service=None, ollama_auto_unload=None, unique_id=None):
        """
        åªåœ¨è¾“å…¥å†…å®¹çœŸæ­£å˜åŒ–æ—¶æ‰è§¦å‘é‡æ–°æ‰§è¡Œ
        ä½¿ç”¨è¾“å…¥å‚æ•°çš„å“ˆå¸Œå€¼ä½œä¸ºåˆ¤æ–­ä¾æ®
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¼ºåˆ¶åˆ·æ–°ç¬¦å· [R]
        if cls._check_is_changed_bypass(source_text):
            return float("nan")

        # è®¡ç®—æ–‡æœ¬çš„å“ˆå¸Œå€¼
        text_hash = ""
        if source_text:
            # ä½¿ç”¨hashlibè®¡ç®—æ–‡æœ¬çš„å“ˆå¸Œå€¼ï¼Œæ›´å®‰å…¨å’Œä¸€è‡´
            text_hash = hashlib.md5(source_text.encode('utf-8')).hexdigest()

        # ç»„åˆæ‰€æœ‰è¾“å…¥çš„å“ˆå¸Œå€¼
        input_hash = hash((
            text_hash,
            target_language,
            translate_service,
            bool(ollama_auto_unload)
        ))

        return input_hash

    def _contains_chinese(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦"""
        if not text:
            return False
        return bool(re.search('[\u4e00-\u9fa5]', text))

    def _detect_language(self, text: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        if not text:
            return "auto"

        # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯è‹±æ–‡ (åªåŒ…å«ASCIIå¯æ‰“å°å­—ç¬¦)
        is_pure_english = bool(re.fullmatch(r'[ -~]+', text))
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        contains_chinese = self._contains_chinese(text)

        if contains_chinese:
            return "zh"
        elif is_pure_english:
            return "en"
        else:
            return "auto"
    
    def translate(self, source_text, target_language, translate_service, ollama_auto_unload, unique_id=None):
        """
        ç¿»è¯‘æ–‡æœ¬å‡½æ•°
        """
        request_id = None  # æå‡åˆ°æ–¹æ³•çº§åˆ«ä½œç”¨åŸŸ
        try:
            # æ£€æŸ¥è¾“å…¥
            if not source_text or not source_text.strip():
                return ("",)

            # è‡ªåŠ¨æ£€æµ‹æºè¯­è¨€
            detected_lang = self._detect_language(source_text)
            to_lang = "en" if target_language == "English" else "zh"

            # æ™ºèƒ½è·³è¿‡ç¿»è¯‘é€»è¾‘
            skip_translation = False
            if to_lang == 'en' and detected_lang == 'en':
                from ..utils.common import _ANSI_CLEAR_EOL
                print(f"\r{_ANSI_CLEAR_EOL}{self.REQUEST_PREFIX} æ£€æµ‹åˆ°è‹±æ–‡è¾“å…¥ï¼Œç›®æ ‡ä¸ºè‹±æ–‡ï¼Œæ— éœ€ç¿»è¯‘", flush=True)
                skip_translation = True
            elif to_lang == 'zh' and detected_lang == 'zh':
                from ..utils.common import _ANSI_CLEAR_EOL
                print(f"\r{_ANSI_CLEAR_EOL}{self.REQUEST_PREFIX} æ£€æµ‹åˆ°ä¸­æ–‡è¾“å…¥ï¼Œç›®æ ‡ä¸ºä¸­æ–‡ï¼Œæ— éœ€ç¿»è¯‘", flush=True)
                skip_translation = True

            if skip_translation:
                return (source_text,)

            # æ˜ å°„è¯­è¨€åç§°
            lang_map = {'zh': 'ä¸­æ–‡', 'en': 'è‹±æ–‡', 'auto': 'åŸæ–‡'}
            from_lang_name = lang_map.get(detected_lang, detected_lang)
            to_lang_name = lang_map.get(to_lang, to_lang)
            
            # ---è§£ææœåŠ¡/æ¨¡å‹å­—ç¬¦ä¸²---
            service_id, model_name = self.parse_service_model(translate_service)
            if not service_id:
                raise ValueError(f"Invalid service selection: {translate_service}")
            
            # ---ç™¾åº¦ç¿»è¯‘ç‰¹æ®Šå¤„ç†---
            if service_id == 'baidu':
                request_id, result = self._translate_with_baidu(source_text, detected_lang, to_lang, translate_service, from_lang_name, to_lang_name, unique_id)
            else:
                # ---LLMç¿»è¯‘:è·å–æœåŠ¡é…ç½®---
                from ..config_manager import config_manager
                service = config_manager.get_service(service_id)
                if not service:
                    raise ValueError(f"Service config not found: {translate_service}")
                
                request_id, result = self._translate_with_llm(source_text, detected_lang, to_lang, service_id, model_name, service, translate_service, from_lang_name, to_lang_name, ollama_auto_unload, unique_id)

            if result and result.get('success'):
                translated_text = result.get('data', {}).get('translated', '').strip()
                if not translated_text:
                    error_msg = 'API returned empty result'
                    raise RuntimeError(f"âŒTranslation failed: {error_msg}")

                # ç»“æœé˜¶æ®µæ—¥å¿—ç”±æœåŠ¡å±‚ç»Ÿä¸€è¾“å‡ºï¼ŒèŠ‚ç‚¹å±‚ä¸å†é‡å¤æ‰“å°
                return (translated_text,)
            else:
                error_msg = result.get('error', 'Unknown error') if result else 'No result returned'
                # å¦‚æœæ˜¯ä¸­æ–­é”™è¯¯,ç›´æ¥æŠ›å‡ºInterruptProcessingException,ä¸æ‰“å°æ—¥å¿—(ç”±åŸºç±»æ‰“å°)
                if error_msg == "ä»»åŠ¡è¢«ä¸­æ–­":
                    raise InterruptProcessingException()
                log_error(TASK_TRANSLATE, request_id, error_msg)
                raise RuntimeError(f"Translation failed: {error_msg}")

        except InterruptProcessingException:
            # ä¸æ‰“å°æ—¥å¿—,ç”±åŸºç±»ç»Ÿä¸€æ‰“å°
            raise
        except Exception as e:
            error_msg = format_api_error(e, translate_service)
            log_error(TASK_TRANSLATE, request_id, error_msg)
            raise RuntimeError(f"Translation error: {error_msg}")

    def _translate_with_baidu(self, text, from_lang, to_lang, service_name, from_lang_name, to_lang_name, unique_id):
        """ä½¿ç”¨ç™¾åº¦ç¿»è¯‘æœåŠ¡"""
        # åˆ›å»ºè¯·æ±‚ID
        request_id = generate_request_id("trans", "baidu", unique_id)
        
        # å‡†å¤‡é˜¶æ®µæ—¥å¿—
        log_prepare(TASK_TRANSLATE, request_id, SOURCE_NODE, "ç™¾åº¦ç¿»è¯‘", None, None, {"æ–¹å‘": f"{from_lang_name}â†’{to_lang_name}", "é•¿åº¦": len(text)})
        
        # æ‰§è¡Œç¿»è¯‘ï¼ˆå¼‚æ­¥çº¿ç¨‹ + å¯ä¸­æ–­ï¼‰
        result = self._run_llm_task(
            BaiduTranslateService.translate,
            service_name,
            text=text,
            from_lang=from_lang,
            to_lang=to_lang,
            request_id=request_id,
            task_type=TASK_TRANSLATE,
            source=SOURCE_NODE
        )

        return request_id, result

    def _translate_with_llm(self, text, from_lang, to_lang, service_id, model_name, service, service_display_name, from_lang_name, to_lang_name, auto_unload, unique_id):
        """ä½¿ç”¨LLMç¿»è¯‘æœåŠ¡"""
        # ---æ„å»ºprovider_config---
        # æŸ¥æ‰¾æŒ‡å®šçš„æ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹
        llm_models = service.get('llm_models', [])
        target_model = None
        
        if model_name:
            # æŸ¥æ‰¾æŒ‡å®šçš„æ¨¡å‹
            target_model = next((m for m in llm_models if m.get('name') == model_name), None)
        
        if not target_model:
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹æˆ–ç¬¬ä¸€ä¸ªæ¨¡å‹
            target_model = next((m for m in llm_models if m.get('is_default')), 
                                llm_models[0] if llm_models else None)
        
        if not target_model:
            return {"success": False, "error": f"Service {service_display_name} has no available models"}
        
        # æ„å»ºé…ç½®å¯¹è±¡
        provider_config = {
            'provider': service_id,
            'model': target_model.get('name', ''),
            'base_url': service.get('base_url', ''),
            'api_key': service.get('api_key', ''),
            'temperature': target_model.get('temperature', 0.7),
            'max_tokens': target_model.get('max_tokens', 1000),
            'top_p': target_model.get('top_p', 0.9),
            'send_temperature': target_model.get('send_temperature', True),
            'send_top_p': target_model.get('send_top_p', True),
            'send_max_tokens': target_model.get('send_max_tokens', True),
            'custom_params': target_model.get('custom_params', ''),
        }
        
        # Ollamaç‰¹æ®Šå¤„ç†:æ·»åŠ auto_unloadé…ç½®
        if service.get('type') == 'ollama':
            provider_config['auto_unload'] = auto_unload

        # åˆ›å»ºè¯·æ±‚ID
        request_id = generate_request_id("trans", "llm", unique_id)
        
        # æ£€æŸ¥æ˜¯å¦å…³é—­æ€ç»´é“¾
        model_full_name = provider_config.get('model')
        disable_thinking_enabled = service.get('disable_thinking', True)
        thinking_extra = build_thinking_suppression(service_id, model_full_name) if disable_thinking_enabled else None
        model_display = format_model_with_thinking(model_full_name, bool(thinking_extra))
        
        # è·å–æœåŠ¡æ˜¾ç¤ºåç§°
        service_display_name = service.get('name', service_id)
        
        # å‡†å¤‡é˜¶æ®µæ—¥å¿—
        log_prepare(TASK_TRANSLATE, request_id, SOURCE_NODE, service_display_name, model_display, None, {"æ–¹å‘": f"{from_lang_name}â†’{to_lang_name}", "é•¿åº¦": len(text)})
        
        # æ£€æŸ¥APIå¯†é’¥å’Œæ¨¡å‹
        api_key = provider_config.get('api_key', '')
        model = provider_config.get('model', '')
        
        if not api_key or not model:
            return {"success": False, "error": f"Please configure API key and model for {service_display_name}"}

        # æ‰§è¡Œç¿»è¯‘ï¼ˆå¼‚æ­¥çº¿ç¨‹ + å¯ä¸­æ–­ï¼‰
        result = self._run_llm_task(
            LLMService.translate,
            service_id,
            text=text,
            from_lang=from_lang,
            to_lang=to_lang,
            request_id=request_id,
            stream_callback=None,
            custom_provider=service_id,
            custom_provider_config=provider_config,
            task_type=TASK_TRANSLATE,
            source=SOURCE_NODE
        )

        return request_id, result


# èŠ‚ç‚¹æ˜ å°„ï¼Œç”¨äºå‘ComfyUIæ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "PromptTranslate": PromptTranslate,
}

# èŠ‚ç‚¹æ˜¾ç¤ºåç§°æ˜ å°„
NODE_DISPLAY_NAME_MAPPINGS = {
    "PromptTranslate": "âœ¨Prompt Translate",
}
